{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing this manually because I hate myself, but not enough to automate it\n",
    "#[directory, filename, fitJSON for t0]\n",
    "#addendum adding an easy way to get the main path correct because I may hate myself, but not the others working on this\n",
    "from os.path import join\n",
    "########CHANGE THIS HERE FOR ANYTHING TO WORK#########\n",
    "#########Excel with compiled data of TA fits and correction values#############\n",
    "excel_path = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\CorrectedPump653ProbeWavelengthScan_UV-extended.xlsx\"\n",
    "#########Path with zheng######################\n",
    "zheng_high_res_path = r\"C:\\Users\\M\\Documents\\Books\\masterprojectinformation\\images\\Zheng2020_higherPrecisionRip.csv\"\n",
    "#########Directory with raw data##############\n",
    "mainDataDirectoryPath = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\"\n",
    "##############-----------------------------------####################\n",
    "#yeah the r\"\\\\\"[0] is an ugly workaround, but it is easy\n",
    "#print(mainDataDirectoryPath+r\"doot\")\n",
    "filePathsOrderedWavScan = []\n",
    "#STD\n",
    "\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"STD-680\"), r\"saturation_2024-01-16_14-09.mat\",  r\"TAfitPump653Probe680_DecayCorrected.JSON\", r\"SummaryPump653Probe680.JSON\"])\n",
    "#print(filePathsOrderedWavScan)\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"STD-520\"), r\"saturation_2024-01-16_15-21.mat\",  r\"TAfitPump653Probe520_DecayCorrected.JSON\", r\"SummaryPump653Probe520.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"STD-502\"), r\"saturation_2024-01-16_14-51.mat\",  r\"TAfitPump653Probe502_DecayCorrected.JSON\", r\"SummaryPump653Probe502.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"STD-493\"), r\"saturation_2024-01-19_14-24.mat\",  r\"TAfitPump653Probe493_DecayCorrected.JSON\", r\"SummaryPump653Probe493Caution.JSON\"])\n",
    "#SHG\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-468\"), r\"saturation_2024-01-16_18-38.mat\",  r\"TAfitPump653Probe468_DecayCorrected.JSON\", r\"SummaryPump653Probe468.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-460\"), r\"saturation_2024-01-18_17-34.mat\",  r\"TAfitPump653Probe460_DecayCorrected.JSON\", r\"SummaryPump653Probe460Caution.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-450\"), r\"saturation_2024-01-18_11-28.mat\",  r\"TAfitPump653Probe450_DecayCorrected.JSON\", r\"SummaryPump653Probe450.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-440\"), r\"saturation_2024-01-16_19-07.mat\",  r\"TAfitPump653Probe440_DecayCorrected.JSON\", r\"SummaryPump653Probe440.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-430\"), r\"saturation_2024-01-18_10-52.mat\",  r\"TAfitPump653Probe430_DecayCorrected.JSON\", r\"SummaryPump653Probe430.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-420\"), r\"saturation_2024-01-17_14-46.mat\",  r\"TAfitPump653Probe420_DecayCorrected.JSON\", r\"SummaryPump653Probe420.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-410\"), r\"saturation_2024-01-19_13-41.mat\",  r\"TAfitPump653Probe410_DecayCorrected.JSON\", r\"SummaryPump653Probe410Caution.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-400\"), r\"saturation_2024-01-17_15-26.mat\",  r\"TAfitPump653Probe400_DecayCorrected.JSON\", r\"SummaryPump653Probe400.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-390\"), r\"saturation_2024-01-18_12-03.mat\",  r\"TAfitPump653Probe390_DecayCorrected.JSON\", r\"SummaryPump653Probe390.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-380\"), r\"saturation_2024-01-17_16-31.mat\",  r\"TAfitPump653Probe380_2_DecayCorrected.JSON\", r\"SummaryPump653Probe380_2.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-370\"), r\"saturation_2024-01-17_17-11.mat\",  r\"TAfitPump653Probe370_DecayCorrected.JSON\", r\"SummaryPump653Probe370.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-360\"), r\"saturation_2024-01-18_15-50.mat\",  r\"TAfitPump653Probe360_DecayCorrected.JSON\", r\"SummaryPump653Probe360_2.JSON\"])\n",
    "filePathsOrderedWavScan.append([join(mainDataDirectoryPath, r\"SHG-350\"), r\"saturation_2024-01-18_16-39.mat\",  r\"TAfitPump653Probe350_DecayCorrected.JSON\", r\"SummaryPump653Probe350.JSON\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting our fitted TA data for every wavelength with different delays\n",
    "Fitting peaks as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot TA probe wavelength spectrum for different delays\n",
    "#Data plotted is taken from fits to the degradation corrected data and is also map corrected\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "import numpy as np\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "from GraphingExcerpt_Functions_TAplotting import paramDictatTime, wavToEnergy, energyToWav, fitPeaks, ErrorCorrectionSimple, multiLorentzianAdditive, multiGaussianAdditive, ErrorCorrectionConvoluted\n",
    "\n",
    "\n",
    "#######################SET VALUES HEREn#######################\n",
    "#delay times in femtoseconds\n",
    "times = np.array([1e2, 1e4, 4e4])\n",
    "#colours of delay times\n",
    "colors = ['red', 'green', 'orange', 'royalblue']\n",
    "#wavelength range for peak fit plotting\n",
    "wav_range = np.linspace(350,550,500)\n",
    "#choose Lorentzian (True) or Gaussian (False) feature shape\n",
    "Lorentz = True\n",
    "#wavelengths at which energies are indicated in nm\n",
    "wavForEnergy = [440, 500, 653]\n",
    "#figure settings\n",
    "fig, ax = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(1,2), dpi = 288)\n",
    "ax.set_xticks(np.arange(300, 701, 50))\n",
    "secax = ax.secondary_xaxis('top', functions=(wavToEnergy, energyToWav))\n",
    "secax.set_xticks(np.round(wavToEnergy(np.array(wavForEnergy)), 2))\n",
    "\n",
    "secax.set_xlabel(r\"$\\mathrm{E_{probe} / eV}$\")\n",
    "ax.set_xlabel('probe wavelength / nm')\n",
    "ax.set_ylabel(r'$\\mathrm{\\Delta A / a.u.}$')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "#######################Camera calibration#######################\n",
    "#This variant obviously only works in case the calibration of the camera is the same for all plotted points\n",
    "umPerPx = 6.949\n",
    "dumPerPx = 4E-2\n",
    "dRelUmPerPx = dumPerPx/umPerPx\n",
    "#######################-----------------#######################\n",
    "\n",
    "#########################PATH HERE NEEDED###################################\n",
    "#excel_path = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\CorrectedPump653ProbeWavelengthScan_UV-extended.xlsx\"\n",
    "SHG_wav_timescan = pd.read_excel(excel_path, sheet_name = \"2024WavelengthScanParameters\", skiprows=[0,1])\n",
    "SHG_main = pd.read_excel(excel_path, sheet_name='2024WavelengthScan', skiprows=[0,1])\n",
    "\n",
    "\n",
    "\n",
    "wavelengths = SHG_main['Probe wavelength / nm']\n",
    "#simple correction without map\n",
    "dOD_SHG_main = SHG_main['cor_dOD / (mOD*m^2/W)']\n",
    "#correction with map\n",
    "dOD_SHG_mapCorr = SHG_main['hypothetical correction']\n",
    "dOD_map_reference = SHG_main['mapReference']\n",
    "dPumpPower = SHG_main['dRelPumpPower']\n",
    "peakRad = SHG_main['Pump power density / (W/m^2)']\n",
    "\n",
    "#map reference correction\n",
    "dOD_map_reference = abs(dOD_map_reference/np.mean(dOD_map_reference))\n",
    "\n",
    "scan_wavs = np.array(SHG_wav_timescan['Probe wavelength / nm'])\n",
    "scan_corrFactors = np.array(SHG_wav_timescan['Correction Factor / (W/m^2)^-1'])\n",
    "scan_params = {\n",
    "    'A1' : SHG_wav_timescan['A1'],\n",
    "    'A2' : SHG_wav_timescan['A2'],\n",
    "    'tau1' : SHG_wav_timescan['tau1'],\n",
    "    'tau2' : SHG_wav_timescan['tau2'],\n",
    "}\n",
    "\n",
    "scan_corrFactors[7] = np.nan\n",
    "#################NEW DUE TO 380 nm measurement being bad, only using one of them##############\n",
    "totalSignalTesting = dOD_map_reference*paramDictatTime(scan_params, 1e2)*scan_corrFactors\n",
    "\n",
    "\n",
    "#get mask of usable data\n",
    "bool_mask = np.invert(np.isnan(totalSignalTesting))\n",
    "\n",
    "yerrCalc = ErrorCorrectionSimple(dOD_map_reference*paramDictatTime(scan_params, 1e2), scan_corrFactors, peakRad, dPumpPower)[bool_mask]\n",
    "\n",
    "#Masking based on what is and what is not usable data\n",
    "totalSignalTesting=totalSignalTesting[bool_mask]\n",
    "dOD_map_reference = dOD_map_reference[bool_mask]\n",
    "scan_corrFactors = scan_corrFactors[bool_mask]\n",
    "peakRad = peakRad[bool_mask]\n",
    "dPumpPower = dPumpPower[bool_mask]\n",
    "scan_wavs = scan_wavs[bool_mask]\n",
    "wavelengths = wavelengths[bool_mask]\n",
    "\n",
    "yerrCalc = np.reshape(yerrCalc, (-1))\n",
    "\n",
    "#print(ErrorCorrectionConvoluted(dOD_SHG_main, scan_corrFactors, peakRad, dRelUmPerPx, dPumpPower))\n",
    "\n",
    "\n",
    "#plot 1e2 fs time errorbar\n",
    "#signalFit = [2.82, 0.14, 1e-3, 2.48, 0.09, 1.3e-3]\n",
    "\n",
    "\n",
    "\n",
    "#signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],2, [2.82, 2.48], [0.1, 0.05],[1e-3, 1e-3])\n",
    "\n",
    "if Lorentz == True:\n",
    "    signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],3, [2.82, 2.48, 3.15], [0.1,0.05, 0.03], [4e-4, 4e-4, 1e-4], positionBounds=[[2.76, 2.83], [2.3, 2.7], [3.00,3.60]], ampBounds=[0,2e-3], function=multiLorentzianAdditive)\n",
    "    #signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],2, [2.82, 2.48], [0.1,0.05], [1e-3, 1e-3], function=multiLorentzianAdditive)\n",
    "else:\n",
    "    signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],3, [2.82, 2.48, 3.55], [0.1,0.05, 0.02], [1e-3, 1e-3, 2e-4], positionBounds=[[2.76, 2.83], [2.3, 2.7], [3.20,3.60]], function=multiGaussianAdditive)\n",
    "\n",
    "#plot gaussians\n",
    "print(signalFit)\n",
    "print(np.sqrt(np.diag(signalCov)))\n",
    "\n",
    "if Lorentz == True:\n",
    "    ax.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit), label=r\"$\\sum$ fits\", color=\"m\", alpha=0.5,linestyle=\"-\")\n",
    "    for i in range(0,len(signalFit),3):\n",
    "        ax.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit[i:i+3]), label=\"fit \"+str(int(i/3)), alpha=0.5,linestyle=\"-\")\n",
    "else:\n",
    "    ax.plot(wav_range, multiGaussianAdditive(wavToEnergy(wav_range), *signalFit), label=\"sum gauss\")\n",
    "    for i in range(0,len(signalFit),3):\n",
    "        ax.plot(wav_range, multiGaussianAdditive(wavToEnergy(wav_range), *signalFit[i:i+3]), label=\"gauss fit \"+str(int(i/3)), linestyle=\"dashed\")\n",
    "\n",
    "#plot the errorbar \n",
    "ax.errorbar(wavelengths, totalSignalTesting, yerr = yerrCalc, ls = \"None\", capsize =2, ecolor=colors[0])\n",
    "\n",
    "'''\n",
    "ax.plot(wavelengths, dOD_SHG_main, 'ro', label='standard correction', ls = \"None\")\n",
    "ax.plot(wavelengths, dOD_SHG_mapCorr, 'bx', label='map corrected', ls = \"None\")\n",
    "#ax.errorbar(wavelengths, dOD_SHG_main, yerr = ErrorCorrectionConvoluted(dOD_SHG_main, scan_corrFactors, peakRad, dRelUmPerPx, dPumpPower), ls = \"None\", capsize =2)\n",
    "'''\n",
    "for ind, time in enumerate(times):\n",
    "    tempVal = dOD_map_reference*paramDictatTime(scan_params, time)[bool_mask]*scan_corrFactors\n",
    "    ax.plot(scan_wavs, tempVal, '1', label=\"%.1f ps\" %(time*1e-3), color = colors[ind])\n",
    "\n",
    "\n",
    "#plot 310 and 330 nm\n",
    "ax.plot([310, 330], [0,0], '1', color = colors[-1])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this still has dependencies on the above cell\n",
    "#run the above cell first as it generates the data, not sure it is worth fully splitting them\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "import numpy as np\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "#######################SET VALUES HEREn#######################\n",
    "#delay times in femtoseconds\n",
    "times = np.array([1e2, 1e5])\n",
    "colors = ['b', 'r']\n",
    "#wavelengths at which energies are indicated in nm\n",
    "wavForEnergy = [440, 500, 653]\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(1,2), dpi = 288)\n",
    "'''\n",
    "ax.plot(wavelengths, dOD_SHG_main, 'ro', label='standard correction', ls = \"None\")\n",
    "ax.plot(wavelengths, dOD_SHG_mapCorr, 'bx', label='map corrected', ls = \"None\")\n",
    "#ax.errorbar(wavelengths, dOD_SHG_main, yerr = ErrorCorrectionConvoluted(dOD_SHG_main, scan_corrFactors, peakRad, dRelUmPerPx, dPumpPower), ls = \"None\", capsize =2)\n",
    "'''\n",
    "\n",
    "ind_color2 = ['royalblue', 'lightcoral']\n",
    "for ind, time in enumerate(times):\n",
    "    #break\n",
    "    ax2.plot(scan_wavs, dOD_map_reference*paramDictatTime(scan_params, time)[bool_mask], colors[ind]+'x', label=\"map corrected: %.1f ps\" %(times[ind]*1e-3))\n",
    "    ax2.plot(scan_wavs, paramDictatTime(scan_params, time)[bool_mask], color = ind_color2[ind], marker= '+', linestyle=\"None\", label=\"assuming homogeniety: %.1f ps\" %(times[ind]*1e-3))\n",
    "''''''\n",
    "ax2.set_xlabel('probe wavelength / nm')\n",
    "ax2.set_ylabel(r'$\\mathrm{\\Delta A / a.u.}$')\n",
    "\n",
    "ax2.set_xticks(np.arange(300, 701, 50))\n",
    "secax = ax2.secondary_xaxis('top', functions=(wavToEnergy, energyToWav))\n",
    "secax.set_xticks(np.round(wavToEnergy(np.array(wavForEnergy)), 2))\n",
    "secax.set_xlabel(r\"$\\mathrm{E_{probe} / eV}$\")\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Zheng data in one plot\n",
    "copied down fitting so this cell should be able to run by its lonesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot TA probe wavelength spectrum for different delays\n",
    "#Data plotted is taken from fits to the degradation corrected data and is also map corrected\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "import numpy as np\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "from GraphingExcerpt_Functions_TAplotting import paramDictatTime, wavToEnergy, energyToWav, fitPeaks, ErrorCorrectionSimple, multiLorentzianAdditive, multiGaussianAdditive, ErrorCorrectionConvoluted\n",
    "\n",
    "\n",
    "#######################SET VALUES HEREn#######################\n",
    "#delay times in femtoseconds\n",
    "times = [2e2, 2.5e3, 1e4, 1e5]\n",
    "#wavelength range for peak fit plotting\n",
    "wav_range = np.linspace(350,550,500)\n",
    "#choose Lorentzian (True) or Gaussian (False) feature shape\n",
    "Lorentz = True\n",
    "#colours for different delay times in order\n",
    "colors = ['red', 'green', 'orange', 'royalblue']\n",
    "#colours used for peak fits\n",
    "color_fits = [\"blue\", \"green\", \"orange\"]\n",
    "#Marker used for Zheng points\n",
    "ZhengMarker = \".\"\n",
    "#Marker used for our data\n",
    "OurMarker = '1'\n",
    "#################Figure settings#####################\n",
    "#wavelengths at which energies are indicated on nm\n",
    "wavForEnergy = [440, 500, 653]\n",
    "#####First plot\n",
    "fig, axs = plt.subplots(1,1, figsize = plotHelperLatex.figSizer(1,2), dpi=288)\n",
    "secax = axs.secondary_xaxis('top', functions=(wavToEnergy, energyToWav))\n",
    "secax.set_xticks(np.round(wavToEnergy(np.array(wavForEnergy)), 2))\n",
    "secax.set_xlabel(r\"$\\mathrm{E_{probe} / eV}$\")\n",
    "axs.set_xlabel('probe wavelength / nm')\n",
    "axs.set_ylabel(r'relative $\\Delta A$ / a.u.')\n",
    "axs.grid(True)\n",
    "#axs.set_ylim((0,1.5))\n",
    "#axs.set_xlim((300, 550))\n",
    "\n",
    "#####Second plot\n",
    "figc, axc = plt.subplots(2,1, figsize = plotHelperLatex.figSizer(1,1), dpi=288)\n",
    "\n",
    "axc[1].set_xlim((300,550))\n",
    "axc[1].set_ylim((0,1.5))\n",
    "\n",
    "#######################Camera calibration#######################\n",
    "#This variant obviously only works in case the calibration of the camera is the same for all plotted points\n",
    "umPerPx = 6.949\n",
    "dumPerPx = 4E-2\n",
    "dRelUmPerPx = dumPerPx/umPerPx\n",
    "#######################-----------------#######################\n",
    "\n",
    "#########################PATH HERE NEEDED###################################\n",
    "#excel_path = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\CorrectedPump653ProbeWavelengthScan_UV-extended.xlsx\"\n",
    "SHG_wav_timescan = pd.read_excel(excel_path, sheet_name = \"2024WavelengthScanParameters\", skiprows=[0,1])\n",
    "SHG_main = pd.read_excel(excel_path, sheet_name='2024WavelengthScan', skiprows=[0,1])\n",
    "\n",
    "\n",
    "\n",
    "wavelengths = SHG_main['Probe wavelength / nm']\n",
    "#simple correction without map\n",
    "dOD_SHG_main = SHG_main['cor_dOD / (mOD*m^2/W)']\n",
    "#correction with map\n",
    "dOD_SHG_mapCorr = SHG_main['hypothetical correction']\n",
    "dOD_map_reference = SHG_main['mapReference']\n",
    "dPumpPower = SHG_main['dRelPumpPower']\n",
    "peakRad = SHG_main['Pump power density / (W/m^2)']\n",
    "\n",
    "#map reference correction\n",
    "dOD_map_reference = abs(dOD_map_reference/np.mean(dOD_map_reference))\n",
    "\n",
    "scan_wavs = np.array(SHG_wav_timescan['Probe wavelength / nm'])\n",
    "scan_corrFactors = np.array(SHG_wav_timescan['Correction Factor / (W/m^2)^-1'])\n",
    "scan_params = {\n",
    "    'A1' : SHG_wav_timescan['A1'],\n",
    "    'A2' : SHG_wav_timescan['A2'],\n",
    "    'tau1' : SHG_wav_timescan['tau1'],\n",
    "    'tau2' : SHG_wav_timescan['tau2'],\n",
    "}\n",
    "\n",
    "scan_corrFactors[7] = np.nan\n",
    "#################NEW DUE TO 380 nm measurement being bad, only using one of them##############\n",
    "totalSignalTesting = dOD_map_reference*paramDictatTime(scan_params, 1e2)*scan_corrFactors\n",
    "\n",
    "\n",
    "#get mask of usable data\n",
    "bool_mask = np.invert(np.isnan(totalSignalTesting))\n",
    "\n",
    "#yerrCalc = ErrorCorrectionSimple(dOD_map_reference*paramDictatTime(scan_params, 1e2), scan_corrFactors, peakRad, dPumpPower)[bool_mask]\n",
    "\n",
    "#Masking based on what is and what is not usable data for fitting\n",
    "totalSignalTesting=totalSignalTesting[bool_mask]\n",
    "wavelengths = wavelengths[bool_mask]\n",
    "\n",
    "#yerrCalc = np.reshape(yerrCalc, (-1))\n",
    "\n",
    "\n",
    "if Lorentz == True:\n",
    "    signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],3, [2.82, 2.48, 3.15], [0.1,0.05, 0.03], [4e-4, 4e-4, 1e-4], positionBounds=[[2.76, 2.83], [2.3, 2.7], [3.00,3.60]], ampBounds=[0,2e-3], function=multiLorentzianAdditive)\n",
    "    #signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],2, [2.82, 2.48], [0.1,0.05], [1e-3, 1e-3], function=multiLorentzianAdditive)\n",
    "else:\n",
    "    signalFit, signalCov = fitPeaks(wavToEnergy(wavelengths[1:-2]), totalSignalTesting[1:-2],3, [2.82, 2.48, 3.55], [0.1,0.05, 0.02], [1e-3, 1e-3, 2e-4], positionBounds=[[2.76, 2.83], [2.3, 2.7], [3.20,3.60]], function=multiGaussianAdditive)\n",
    "\n",
    "#plot gaussians\n",
    "print(signalFit)\n",
    "print(np.sqrt(np.diag(signalCov)))\n",
    "###########################################################################################################################################\n",
    "###########################################################################################################################################\n",
    "############################Above is mainly the dependency for singalfit, below is the actual code for the comparison######################\n",
    "###########################################################################################################################################\n",
    "###########################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "wavelengths = SHG_main['Probe wavelength / nm']\n",
    "#simple correction without map\n",
    "dOD_SHG_main = SHG_main['cor_dOD / (mOD*m^2/W)']\n",
    "#correction with map\n",
    "dOD_SHG_mapCorr = SHG_main['hypothetical correction']\n",
    "dOD_map_reference = abs(SHG_main['mapReference'])\n",
    "dPumpPower = SHG_main['dRelPumpPower']\n",
    "peakRad = SHG_main['Pump power density / (W/m^2)']\n",
    "\n",
    "\n",
    "scan_wavs = SHG_wav_timescan['Probe wavelength / nm']\n",
    "scan_corrFactors = SHG_wav_timescan['Correction Factor / (W/m^2)^-1']\n",
    "\n",
    "\n",
    "#high res zheng\n",
    "################################Fix the path here to \"high resolution\" zheng data -> do in top cell################################\n",
    "#zheng_high_res_path = r\"C:\\Users\\M\\Documents\\Books\\masterprojectinformation\\images\\Zheng2020_higherPrecisionRip.csv\"\n",
    "Zheng = pd.read_csv(zheng_high_res_path, sep=\",\")\n",
    "################################Manually choose the different points from Zheng you want, look at csv data for possible values################\n",
    "ZhengDict = {}\n",
    "ZhengDict['200fs'] = np.array([Zheng[\"0.2 ps\"][1:], Zheng[\"Y0.2 ps\"][1:]], dtype=float)\n",
    "ZhengDict['2500fs'] = np.array([Zheng[\"2.5 ps\"][1:], Zheng[\"Y2.5 ps\"][1:]], dtype=float)\n",
    "ZhengDict['10ps'] = np.array([Zheng[\"10 ps\"][1:], Zheng[\"Y10 ps\"][1:]], dtype=float)\n",
    "ZhengDict['100ps'] = np.array([Zheng[\"100 ps\"][1:], Zheng[\"Y100 ps\"][1:]], dtype=float)\n",
    "#print(np.array(ZhengDict[\"2500fs\"][0,:],dtype=int))\n",
    "#print(np.array(ZhengDict[\"2500fs\"][0,:],dtype=int) == 680)\n",
    "ZhengModifier = abs(ZhengDict[\"2500fs\"][1,np.array(ZhengDict[\"2500fs\"][0,:],dtype=int) == 679])\n",
    "\n",
    "\n",
    "\n",
    "indexMeasureModifier = np.array(scan_wavs, int) == 680\n",
    "modifier_scan_params = {\n",
    "    'A1' : SHG_wav_timescan['A1'][indexMeasureModifier],\n",
    "    'A2' : SHG_wav_timescan['A2'][indexMeasureModifier],\n",
    "    'tau1' : SHG_wav_timescan['tau1'][indexMeasureModifier],\n",
    "    'tau2' : SHG_wav_timescan['tau2'][indexMeasureModifier],\n",
    "}\n",
    "\n",
    "#Modifier used for scaling correctly\n",
    "measurementModifier = abs(scan_corrFactors*dOD_map_reference[indexMeasureModifier]*paramDictatTime(modifier_scan_params, times[0]))[0]\n",
    "\n",
    "if True:\n",
    "    sizeM = 2\n",
    "    for ind, key in enumerate(ZhengDict):\n",
    "        axs.plot(ZhengDict[key][0,:], ZhengDict[key][1,:]/ZhengModifier, linestyle=\"None\", marker = ZhengMarker, markersize= sizeM, color=colors[ind])\n",
    "        axc[0].plot(ZhengDict[key][0,:], ZhengDict[key][1,:]/ZhengModifier, linestyle=\"None\", marker = ZhengMarker, markersize= sizeM, color=colors[ind])\n",
    "        axc[1].plot(ZhengDict[key][0,:], ZhengDict[key][1,:]/ZhengModifier, linestyle=\"None\", marker = ZhengMarker, markersize= sizeM, color=colors[ind])\n",
    "else:\n",
    "    sizeM = 1\n",
    "    for ind, key in enumerate(ZhengDict):\n",
    "        axs.plot(ZhengDict[key][0,:], ZhengDict[key][1,:]/ZhengModifier, linestyle=\"-\", linewidth = sizeM, marker = \"None\", color=colors[ind])\n",
    "        axc[0].plot(ZhengDict[key][0,:], ZhengDict[key][1,:]/ZhengModifier, linestyle=\"-\", linewidth = sizeM,  marker = \"None\", color=colors[ind])\n",
    "        axc[1].plot(ZhengDict[key][0,:], ZhengDict[key][1,:]/ZhengModifier, linestyle=\"-\", linewidth = sizeM,  marker = \"None\", color=colors[ind])\n",
    "\n",
    "for ind, time in enumerate(times):\n",
    "    tempVal = scan_corrFactors*dOD_map_reference*paramDictatTime(scan_params, time)/measurementModifier\n",
    "\n",
    "    axs.plot(scan_wavs, tempVal, OurMarker, color = colors[ind], label=\"%.1f ps\" %(time*1e-3))\n",
    "    axc[0].plot(scan_wavs, tempVal, OurMarker, color = colors[ind], label=\"%.1f ps\" %(time*1e-3))\n",
    "    axc[1].plot(scan_wavs, tempVal, OurMarker, color = colors[ind], label=\"%.1f ps\" %(time*1e-3))\n",
    "#plotting 310 and 330 by hand\n",
    "axs.plot([310, 330], [0,0], OurMarker, color = colors[-1], label=\"%.1f ps\" %(times[-1]*1e-3))\n",
    "axc[0].plot([310, 330], [0,0], OurMarker, color = colors[-1], label=\"%.1f ps\" %(times[-1]*1e-3))\n",
    "axc[1].plot([310, 330], [0,0], OurMarker, color = colors[-1], label=\"%.1f ps\" %(times[-1]*1e-3))\n",
    "\n",
    "#############plot the fitting of with cauchy functions##############################\n",
    "y_dataFit = scan_corrFactors*dOD_map_reference*paramDictatTime(scan_params, times[0])/measurementModifier\n",
    "y_dataFit = y_dataFit[1:-2]\n",
    "wav_dataFit = scan_wavs[1:-2]\n",
    "#do nan-check\n",
    "#nanBool = np.isnan(y_dataFit)\n",
    "\n",
    "\n",
    "\n",
    "######################Not sure why the division by measurementModifier (does normalisation) is missing for these, divide the y data of the fits by it and you should get a good output############\n",
    "#I opted to do it for you, since it should have been the case anyways. Not sure why it was not at the time I am going through this again.\n",
    "axs.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit)/measurementModifier, label=r\"$\\sum$ fits\", alpha=0.5,linestyle=\"-\", color = \"m\")\n",
    "axc[1].plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit)/measurementModifier, label=r\"$\\sum$ fits\", alpha=0.5,linestyle=\"-\", color = \"m\")\n",
    "for i in range(0,len(signalFit),3):\n",
    "    #break\n",
    "    axs.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit[i:i+3])/measurementModifier, label=\"fit \"+str(int(i/3)), alpha=0.5,linestyle=\"-\", color = color_fits[int(i/3)])\n",
    "    axc[1].plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit[i:i+3])/measurementModifier, label=\"fit \"+str(int(i/3)), alpha=0.5,linestyle=\"-\", color = color_fits[int(i/3)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_items = [Line2D([0], [0], color = \"black\", label=\"Zheng et al.\", marker=\".\", linestyle=\"None\"),\n",
    "                Line2D([0], [0], color = \"black\", label=\"SHG setup\", marker=\"1\",linestyle=\"None\")]\n",
    "for ind in range(len(times)):\n",
    "    legend_items.append(Line2D([0], [0], color = colors[ind], label=\"%.1f ps\" %(times[ind]*1e-3), marker=\"s\", linestyle=\"None\"))\n",
    "\n",
    "#introduce cauchy fits to legend\n",
    "#legend_items.append(Line2D([0], [0], label=r\"$\\sum$ fits\", linestyle=\"-\", alpha=0.5, color = \"m\"))\n",
    "for l2d in range(3):\n",
    "    break\n",
    "    legend_items.append(Line2D([0], [0], label=\"fit \"+str(int(l2d)), linestyle=\"-\", alpha=0.5, color = color_fits[l2d]))\n",
    "axs.legend(handles=legend_items)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    secax2 = axc[i].secondary_xaxis('top', functions=(wavToEnergy, energyToWav))\n",
    "    secax2.set_xticks(np.round(wavToEnergy(np.array(wavForEnergy)), 2))\n",
    "    secax2.set_xlabel(r\"$\\mathrm{E_{probe} / eV}$\")\n",
    "    axc[i].set_xlabel('probe wavelength / nm')\n",
    "    axc[i].set_ylabel(r'relative $\\Delta A$ / a.u.')\n",
    "    #axc[i].legend(handles=legend_items)\n",
    "    axc[i].grid(visible=True)\n",
    "axc[1].legend(handles=legend_items)\n",
    "\n",
    "\n",
    "\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "####################################Moving mean to emulate spectral width of probe (rudimentary)########################\n",
    "#attempt the alternative moving mean at a width of 10 nm aka +- 5 nm\n",
    "gaussian = lambda wavs, centerwav: 1/np.sqrt(2*np.pi)/5*np.exp(-(centerwav-wavs)**2/(2*5**2))\n",
    "ZhengOurWavelengths = np.zeros((len(ZhengDict.keys()), len(wavelengths)))\n",
    "for wav_ind, wav in enumerate(wavelengths):\n",
    "    for key_ind, key in enumerate(ZhengDict.keys()):\n",
    "        #print(key)\n",
    "        wav_bool_array = (wav- 30< ZhengDict[key][0,:]) & (wav +30 > ZhengDict[key][0,:])\n",
    "        #bool_array = (delays[indWav,:] > plotDelay[indDelay]) & (delays[indWav,:] < plotDelay[indDelay] + 1e3/(n_subticks))\n",
    "        #ZhengOurWavelengths[key_ind, wav_ind] = np.sum(ZhengDict[key][1,wav_bool_array])\n",
    "        ZhengOurWavelengths[key_ind, wav_ind] = np.sum(gaussian(ZhengDict[key][0,wav_bool_array], wav)*ZhengDict[key][1,wav_bool_array])\n",
    "        #print(gaussian(ZhengDict[key][0,:], wav)*ZhengDict[key][1,:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single delay time comparison to Zheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.constants as const\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "from GraphingExcerpt_Functions_TAplotting import paramDictatTime, wavToEnergy, energyToWav, fitPeaks, ErrorCorrectionSimple, multiLorentzianAdditive, multiGaussianAdditive, ErrorCorrectionConvoluted\n",
    "\n",
    "########################################\n",
    "#Colours of fits our setup\n",
    "color_fits = [\"blue\", \"green\", \"purple\"]\n",
    "#colours of fits Zheng\n",
    "colors = ['red', 'green', 'orange', 'royalblue']\n",
    "#wavelength range to plot fits\n",
    "wav_range = np.linspace(350,550,500)\n",
    "#delay time of data in fs (note this is fit data as per usual, in this case both for zheng and our data)\n",
    "times = [10e3]\n",
    "#wavelengths at which energies are indicated in nm\n",
    "wavForEnergy = [410,440, 500, 653]\n",
    "\n",
    "#excel_path = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\CorrectedPump653ProbeWavelengthScan_UV-extended.xlsx\"\n",
    "SHG_wav_timescan = pd.read_excel(excel_path, sheet_name = \"2024WavelengthScanParameters\", skiprows=[0,1])\n",
    "SHG_main = pd.read_excel(excel_path, sheet_name='2024WavelengthScan', skiprows=[0,1])\n",
    "#print(SHG_main)\n",
    "#print(SHG_main.keys())\n",
    "\n",
    "\n",
    "wavelengths = SHG_main['Probe wavelength / nm']\n",
    "#simple correction without map\n",
    "dOD_SHG_main = SHG_main['cor_dOD / (mOD*m^2/W)']\n",
    "#correction with map\n",
    "dOD_SHG_mapCorr = SHG_main['hypothetical correction']\n",
    "dOD_map_reference = abs(SHG_main['mapReference'])\n",
    "dPumpPower = SHG_main['dRelPumpPower']\n",
    "peakRad = SHG_main['Pump power density / (W/m^2)']\n",
    "\n",
    "\n",
    "scan_wavs = SHG_wav_timescan['Probe wavelength / nm']\n",
    "scan_corrFactors = SHG_wav_timescan['Correction Factor / (W/m^2)^-1']\n",
    "scan_params = {\n",
    "    'A1' : SHG_wav_timescan['A1'],\n",
    "    'A2' : SHG_wav_timescan['A2'],\n",
    "    'tau1' : SHG_wav_timescan['tau1'],\n",
    "    'tau2' : SHG_wav_timescan['tau2'],\n",
    "}\n",
    "\n",
    "scan_corrFactors[7] = np.nan\n",
    "#high res zheng\n",
    "\n",
    "#zheng_high_res_path = r\"C:\\Users\\M\\Documents\\Books\\masterprojectinformation\\images\\Zheng2020_higherPrecisionRip.csv\"\n",
    "Zheng = pd.read_csv(zheng_high_res_path, sep=\",\")\n",
    "\n",
    "ZhengDict = {}\n",
    "ZhengDict['200fs'] = np.array([Zheng[\"0.2 ps\"][1:], Zheng[\"Y0.2 ps\"][1:]], dtype=float)\n",
    "ZhengDict['2500fs'] = np.array([Zheng[\"2.5 ps\"][1:], Zheng[\"Y2.5 ps\"][1:]], dtype=float)\n",
    "ZhengDict['10ps'] = np.array([Zheng[\"10 ps\"][1:], Zheng[\"Y10 ps\"][1:]], dtype=float)\n",
    "ZhengDict['100ps'] = np.array([Zheng[\"100 ps\"][1:], Zheng[\"Y100 ps\"][1:]], dtype=float)\n",
    "#print(np.array(ZhengDict[\"2500fs\"][0,:],dtype=int))\n",
    "#print(np.array(ZhengDict[\"2500fs\"][0,:],dtype=int) == 680)\n",
    "ZhengModifier = abs(ZhengDict[\"2500fs\"][1,np.array(ZhengDict[\"2500fs\"][0,:],dtype=int) == 679])\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize = plotHelperLatex.figSizer(1,2), dpi=288)\n",
    "#figc, axc = plt.subplots(2,1, figsize = plotHelperLatex.figSizer(1, 1.4), dpi=288)\n",
    "\n",
    "#times = [2e2, 2.5e3, 1e4, 1e5]\n",
    "\n",
    "\n",
    "\n",
    "indexMeasureModifier = np.array(scan_wavs, int) == 680\n",
    "modifier_scan_params = {\n",
    "    'A1' : SHG_wav_timescan['A1'][indexMeasureModifier],\n",
    "    'A2' : SHG_wav_timescan['A2'][indexMeasureModifier],\n",
    "    'tau1' : SHG_wav_timescan['tau1'][indexMeasureModifier],\n",
    "    'tau2' : SHG_wav_timescan['tau2'][indexMeasureModifier],\n",
    "}\n",
    "ZhengRefSig = np.array(Zheng[\"Y10 ps\"][1:], dtype=float)\n",
    "ZhengRefWav = np.array(Zheng[\"10 ps\"][1:], dtype=float)\n",
    "bool_mask = np.invert(np.isnan(ZhengRefSig))\n",
    "ZhengRefSig = np.array(ZhengRefSig[bool_mask])\n",
    "ZhengRefWav = ZhengRefWav[bool_mask]\n",
    "\n",
    "measurementModifier = abs(scan_corrFactors*dOD_map_reference[indexMeasureModifier]*paramDictatTime(modifier_scan_params, times[0]))[0]\n",
    "\n",
    "zhengFit, zhengCov = fitPeaks(wavToEnergy(ZhengRefWav), ZhengRefSig/ZhengModifier,2, [2.5, 2.75], [0.1,0.1], [1, 1], positionBounds=[[2, 3], [2, 3]], function=multiLorentzianAdditive, sigBounds=[[0.02,0.9], [0.1,np.inf]], ampBounds=[0.4, 2])\n",
    "#zhengFit, zhengCov = fitPeaks(wavToEnergy(ZhengRefWav), ZhengRefSig/ZhengModifier,2, [2.82, 2.48], [0.2,0.05], [0.35, 0.7], positionBounds=[[2.76, 2.83], [2.3, 2.7]], function=multiGaussianAdditive)\n",
    "print(zhengFit)\n",
    "\n",
    "axs.plot(ZhengRefWav, ZhengRefSig/ZhengModifier, linestyle=\"None\", marker = \".\", markersize= 2, color=\"r\")\n",
    "\n",
    "\n",
    "\n",
    "for ind, time in enumerate(times):\n",
    "    axs.plot(scan_wavs, scan_corrFactors*dOD_map_reference*paramDictatTime(scan_params, time)/measurementModifier, '1', color = \"b\", label=\"%.1f ps\" %(time*1e-3))\n",
    "\n",
    "#plotting 310 and 330 by hand\n",
    "axs.plot([310, 330], [0,0], '1', color = \"b\")\n",
    "\n",
    "\n",
    "#############plot the fitting of with cauchy functions##############################\n",
    "y_dataFit = scan_corrFactors*dOD_map_reference*paramDictatTime(scan_params, times[0])/measurementModifier\n",
    "y_dataFit = y_dataFit[1:-2]\n",
    "wav_dataFit = scan_wavs[1:-2]\n",
    "#get rid of nan\n",
    "bool_mask = np.invert(np.isnan(y_dataFit))\n",
    "y_dataFit = np.array(y_dataFit[bool_mask])\n",
    "wav_dataFit = wav_dataFit[bool_mask]\n",
    "\n",
    "signalFit, signalCov = fitPeaks(wavToEnergy(wav_dataFit), y_dataFit, 3, [2.75, 2.48, 3.15], [0.1,0.04, 0.02], [0.5, 0.5, 0.1], positionBounds=[[2.74, 2.83], [2.3, 2.7], [3.015,3.60]], function=multiLorentzianAdditive, ampBounds=[0,np.inf])\n",
    "\n",
    "\n",
    "\n",
    "axs.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit), label=r\"$\\sum$ fits\", alpha=0.5,linestyle=\"-\", color = \"b\")\n",
    "axs.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *zhengFit), label=r\"$\\sum$ fits\", alpha=0.5,linestyle=\"-\", color = \"r\")\n",
    "#axs.plot(wav_range, multiGaussianAdditive(wavToEnergy(wav_range), *zhengFit), label=r\"$\\sum$ fits\", alpha=0.5,linestyle=\"-\", color = \"r\")\n",
    "\n",
    "for i in range(0,len(signalFit),3):\n",
    "    #break\n",
    "    axs.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *signalFit[i:i+3]), label=\"fit \"+str(int(i/3)), alpha=0.3,linestyle=\"--\", color = \"b\")\n",
    "for i in range(0,len(zhengFit),3):\n",
    "    #axs.plot(wav_range, multiGaussianAdditive(wavToEnergy(wav_range), *zhengFit[i:i+3]), label=\"fit \"+str(int(i/3)), alpha=0.3,linestyle=\"--\", color = \"r\")\n",
    "    axs.plot(wav_range, multiLorentzianAdditive(wavToEnergy(wav_range), *zhengFit[i:i+3]), label=\"fit \"+str(int(i/3)), alpha=0.3,linestyle=\"--\", color = \"r\")\n",
    "secax = axs.secondary_xaxis('top', functions=(wavToEnergy, energyToWav))\n",
    "secax.set_xticks(np.round(wavToEnergy(np.array(wavForEnergy)), 2))\n",
    "secax.set_xlabel(r\"$\\mathrm{E_{probe} / eV}$\")\n",
    "\n",
    "\n",
    "\n",
    "axs.set_xlim((300,550))\n",
    "axs.set_ylim((0,1.2))\n",
    "axs.set_xlabel('probe wavelength / nm')\n",
    "axs.set_ylabel(r'relative $\\Delta A$ / a.u.')\n",
    "axs.grid(True)\n",
    "\n",
    "print(signalFit)\n",
    "\n",
    "#custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_single = [Line2D([0], [0], color = \"r\", label=\"Zheng et al.\", marker=\".\", linestyle=\"None\"),\n",
    "                    Line2D([0], [0], color = \"r\", label=r\"Zheng $\\sum$ fits\", marker=\"None\",linestyle=\"-\", alpha=0.5),\n",
    "                    Line2D([0], [0], color = \"r\", label=r\"Zheng fits\", marker=\"None\",linestyle=\"--\", alpha=0.3),\n",
    "                    Line2D([0], [0], color = \"b\", label=\"SHG setup\", marker=\"1\",linestyle=\"None\"), \n",
    "                    Line2D([0], [0], color = \"b\", label=r\"SHG $\\sum$ fits\", marker=\"None\",linestyle=\"-\", alpha=0.5),\n",
    "                    Line2D([0], [0], color = \"b\", label=r\"SHG fits\", marker=\"None\",linestyle=\"--\", alpha=0.3)]\n",
    "\n",
    "axs.legend(handles=legend_single)\n",
    "legend_itemsZheng = [Line2D([0], [0], color = \"r\", label=\" 2.5 ps Zheng et al.\", marker=\".\", linestyle=\"None\")]\n",
    "legend_itemsSHG = [Line2D([0], [0], color = \"r\", label=\"2.5 ps SHG setup\", marker=\"1\",linestyle=\"None\")]\n",
    "for ind in range(len(times)):\n",
    "    break\n",
    "    legend_items.append(Line2D([0], [0], color = colors[ind], label=\"%.1f ps\" %(times[ind]*1e-3), marker=\"s\", linestyle=\"None\"))\n",
    "\n",
    "#introduce cauchy fits to legend\n",
    "legend_itemsZheng.append(Line2D([0], [0], label=r\"$\\sum$ fits\", linestyle=\"-\", alpha=0.5, color = \"m\"))\n",
    "legend_itemsSHG.append(Line2D([0], [0], label=r\"$\\sum$ fits\", linestyle=\"-\", alpha=0.5, color = \"m\"))\n",
    "for l2d in range(3):\n",
    "    #break\n",
    "    legend_itemsSHG.append(Line2D([0], [0], label=\"fit \"+str(int(l2d)), linestyle=\"-\", alpha=0.5, color = color_fits[l2d]))\n",
    "    if l2d < 2:\n",
    "        legend_itemsZheng.append(Line2D([0], [0], label=\"fit \"+str(int(l2d)), linestyle=\"-\", alpha=0.5, color = color_fits[l2d]))\n",
    "#axs.legend(handles=legend_items)\n",
    "\n",
    "\n",
    "\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots a 2d heatmap of the TA delayfits at every wavelength measured\n",
    "You already have a preferred way of plotting this, so I am keeping it as is and you just grab the way the data is extracted and put your own preferred fitting option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this here is supposed to show 'raw' data over wavelength\n",
    "#copied most from above\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "import numpy as np\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "import fileParsingMethods\n",
    "import SeriesDegradation\n",
    "import json\n",
    "\n",
    "umPerPx = 6.949\n",
    "dumPerPx = 4E-2\n",
    "dRelUmPerPx = dumPerPx/umPerPx\n",
    "\n",
    "def wavToEnergy(lamb):\n",
    "    #Energy in eV, lamb in nm\n",
    "    return const.h*const.c/lamb*1e9/const.elementary_charge\n",
    "\n",
    "def energyToWav(E):\n",
    "    #Energy in eV, lamb in nm\n",
    "    return const.h*const.c/E*1e9/const.elementary_charge\n",
    "\n",
    "def paramDictatTime(param_dict, time_fs):\n",
    "    return param_dict['A1']*np.exp(-time_fs/param_dict['tau1']) + param_dict['A2']*np.exp(-time_fs/param_dict['tau2'])\n",
    "#import data from excel (terrible choice I know, but I want the visualisation and comparability)\n",
    "\n",
    "def ErrorCorrectionConvoluted(signal, correctionFactor, peakRadiance, dRelUmPerPx, dRelPower):\n",
    "    '''correctionfactor is already divided with peak radiance\\\\\n",
    "    dRelPower = dPower/Power'''\n",
    "    dPeakRadiance = dRelPower*peakRadiance + 2*peakRadiance*dRelUmPerPx\n",
    "    return abs(signal*correctionFactor/peakRadiance*dPeakRadiance)\n",
    "\n",
    "#doing this manually because I hate myself, but not enough to automate it\n",
    "#[directory, filename, fitJSON for t0]\n",
    "filePaths = filePathsOrderedWavScan\n",
    "r\"\"\"\n",
    "#STD\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\STD-680\", r\"saturation_2024-01-16_14-09.mat\",  r\"TAfitPump653Probe680_DecayCorrected.JSON\", r\"SummaryPump653Probe680.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\STD-520\", r\"saturation_2024-01-16_15-21.mat\",  r\"TAfitPump653Probe520_DecayCorrected.JSON\", r\"SummaryPump653Probe520.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\STD-502\", r\"saturation_2024-01-16_14-51.mat\",  r\"TAfitPump653Probe502_DecayCorrected.JSON\", r\"SummaryPump653Probe502.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\STD-493\", r\"saturation_2024-01-19_14-24.mat\",  r\"TAfitPump653Probe493_DecayCorrected.JSON\", r\"SummaryPump653Probe493Caution.JSON\"])\n",
    "#SHG\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-468\", r\"saturation_2024-01-16_18-38.mat\",  r\"TAfitPump653Probe468_DecayCorrected.JSON\", r\"SummaryPump653Probe468.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-460\", r\"saturation_2024-01-18_17-34.mat\",  r\"TAfitPump653Probe460_DecayCorrected.JSON\", r\"SummaryPump653Probe460Caution.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-450\", r\"saturation_2024-01-18_11-28.mat\",  r\"TAfitPump653Probe450_DecayCorrected.JSON\", r\"SummaryPump653Probe450.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-440\", r\"saturation_2024-01-16_19-07.mat\",  r\"TAfitPump653Probe440_DecayCorrected.JSON\", r\"SummaryPump653Probe440.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-430\", r\"saturation_2024-01-18_10-52.mat\",  r\"TAfitPump653Probe430_DecayCorrected.JSON\", r\"SummaryPump653Probe430.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-420\", r\"saturation_2024-01-17_14-46.mat\",  r\"TAfitPump653Probe420_DecayCorrected.JSON\", r\"SummaryPump653Probe420.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-410\", r\"saturation_2024-01-19_13-41.mat\",  r\"TAfitPump653Probe410_DecayCorrected.JSON\", r\"SummaryPump653Probe410Caution.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-400\", r\"saturation_2024-01-17_15-26.mat\",  r\"TAfitPump653Probe400_DecayCorrected.JSON\", r\"SummaryPump653Probe400.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-390\", r\"saturation_2024-01-18_12-03.mat\",  r\"TAfitPump653Probe390_DecayCorrected.JSON\", r\"SummaryPump653Probe390.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-380\", r\"saturation_2024-01-17_16-31.mat\",  r\"TAfitPump653Probe380_2_DecayCorrected.JSON\", r\"SummaryPump653Probe380_2.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-370\", r\"saturation_2024-01-17_17-11.mat\",  r\"TAfitPump653Probe370_DecayCorrected.JSON\", r\"SummaryPump653Probe370.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-360\", r\"saturation_2024-01-18_15-50.mat\",  r\"TAfitPump653Probe360_DecayCorrected.JSON\", r\"SummaryPump653Probe360_2.JSON\"])\n",
    "filePaths.append([r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-350\", r\"saturation_2024-01-18_16-39.mat\",  r\"TAfitPump653Probe350_DecayCorrected.JSON\", r\"SummaryPump653Probe350.JSON\"])\n",
    "\"\"\"\n",
    "#import map correction\n",
    "#excel_path = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\CorrectedPump653ProbeWavelengthScan_UV-extended.xlsx\"\n",
    "SHG_main = pd.read_excel(excel_path, sheet_name='2024WavelengthScan', skiprows=[0,1], usecols=[\"Probe wavelength / nm\", \"mapReference\"])\n",
    "\n",
    "SHG_main = SHG_main.sort_values(by=\"Probe wavelength / nm\", ascending=False)\n",
    "SHG_main.drop_duplicates()\n",
    "SHG_main= SHG_main[:-2]\n",
    "#print(SHG_main)\n",
    "def updateFitJSON(fitJSONpath,summaryJSONpath, referencePower, referenceDecay):\n",
    "    return\n",
    "    #I am pretty sure you don't want to use this accidentally. I am no longer sure when it had a purpose but I had to fix something with the JSONs at some point\n",
    "    #beware this only works for single entries\n",
    "    fitJSONfile = open(fitJSONpath)\n",
    "    fitJSON = json.load(fitJSONfile)\n",
    "    fitJSONfile.close()\n",
    "    sumJSONfile = open(summaryJSONpath)\n",
    "    sumJSON = json.load(sumJSONfile)\n",
    "    sumJSONfile.close()\n",
    "    ownPower = sumJSON[\"Pump power density: W/m^2\"]\n",
    "    fitJSON[\"entries\"][0][\"degradePowerRatio\"] = ownPower/referencePower\n",
    "    fitJSON[\"entries\"][0][\"tauExpDegrade\"] = referenceDecay\n",
    "    #print(fitJSON)\n",
    "    fitJSONfile = open(fitJSONpath, 'w')\n",
    "    json.dump(fitJSON,fitJSONfile)\n",
    "    fitJSONfile.truncate()\n",
    "    fitJSONfile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dOD_map_reference = SHG_main[\"mapReference\"]\n",
    "dOD_map_reference = abs(dOD_map_reference/np.mean(dOD_map_reference))\n",
    "pRatioReference = 1575\n",
    "#now grab the data, correct/background substract it it and stuff it into an array as a mean\n",
    "wav_vec = [680, 520, 502, 493, 468, 460, 450, 440, 430, 420, 410, 400, 390, 380, 370, 360, 350]\n",
    "bgLen = np.ones(np.shape(wav_vec), dtype=int)*25\n",
    "#the linear background ones need more precise backgroundlengths\n",
    "bgLen[[0,1,2,6,8]] = [40, 30, 25, 35, 35]\n",
    "data_raw = []\n",
    "delays = []\n",
    "t0 = []\n",
    "for ind, path_list in enumerate(filePaths):\n",
    "    #load json with data needed\n",
    "    #overlap JSON\n",
    "    json_file = open(path_list[0] + r\"\\\\\"[0] + path_list[3])\n",
    "    overlapCorrection = json.load(json_file)[\"Correction Factor: per W/m^2 pump power\"]\n",
    "    #fit JSON\n",
    "    #update if needed first\n",
    "    #updateFitJSON(path_list[0] + r\"\\\\\"[0] + path_list[2], path_list[0] + r\"\\\\\"[0] + path_list[3], pRatioReference, 12600)\n",
    "    json_file = open(path_list[0] + r\"\\\\\"[0] + path_list[2])\n",
    "    temp_JSON = json.load(json_file)[\"entries\"][0]\n",
    "\n",
    "    \n",
    "    #print(temp_JSON)\n",
    "    #linBgBool is for if you want a linear background subtraction or not. \n",
    "    if ('bgParam' in temp_JSON.keys()) and (temp_JSON['bgParam'][0] != 0):\n",
    "        linBgBool = True\n",
    "    else:\n",
    "        linBgBool = False\n",
    "    \n",
    "    tempdata, _, degradeFactors, tempdelay, _ = SeriesDegradation.autoCompensation(path_list[1], dirPath = path_list[0], degConst = temp_JSON[\"tauExpDegrade\"], p_ratio = temp_JSON[\"degradePowerRatio\"], linearBg=linBgBool, backgroundMean=bgLen[ind])\n",
    "    data_raw.append(np.mean(overlapCorrection*dOD_map_reference[ind]*tempdata*degradeFactors, axis = 0))\n",
    "    t0.append(temp_JSON[\"startParameters\"][0][\"position\"])\n",
    "    #print(t0)\n",
    "    delays.append(tempdelay- t0[ind])\n",
    "\n",
    "delays = np.array(delays)\n",
    "data_raw = np.array(data_raw)\n",
    "\n",
    "#create my own average time delay with n steps per picosecond and calculate the average value for plotting\n",
    "n_subticks = int(4)\n",
    "plotDelay = np.linspace(-1e4, 3e4, 40*n_subticks)\n",
    "plotData = np.zeros((len(wav_vec), len(plotDelay)))\n",
    "\n",
    "for indWav in range(len(wav_vec)):\n",
    "    '''\n",
    "    for indDelay in range(len(plotDelay)):\n",
    "        delay_boolArray = (delays[indWav,:] > plotDelay[indDelay]) & (delays[indWav,:] < plotDelay[indDelay] + 1e3/(n_subticks))\n",
    "        #print(delay_boolArray)\n",
    "        plotData[indWav,indDelay] = np.mean(data_raw[indWav, delay_boolArray], axis = None)\n",
    "    '''\n",
    "    #interpolate instead\n",
    "    plotData[indWav, :] = np.interp(plotDelay, delays[indWav,:],data_raw[indWav, :])\n",
    "\n",
    "\n",
    "#print(data_raw)\n",
    "plotData = plotData/np.max(abs(plotData))\n",
    "delayGrid, wavGrid = np.meshgrid(plotDelay, np.arange(0,len(wav_vec), 1))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(1), dpi=288)\n",
    "map = ax.pcolor(delayGrid*1e-3, wavGrid, plotData, cmap=\"bwr\", clim=(-1,+1))\n",
    "#check out pcolormesh\n",
    "#map = ax.pcolormesh(delayGrid*1e-3, wavGrid, plotData, cmap=\"bwr\", clim=(-1,+1), shading =\"gouraud\")\n",
    "fig.colorbar(map, ax = ax, label=r\"relative $\\Delta A$ / a.u.\")\n",
    "ax.set_yticks(np.arange(0,len(wav_vec), 1), wav_vec)\n",
    "ax.set_ylabel('probe wavelength / nm')\n",
    "ax.set_xlabel('delay / ps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example single TA fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows the fit with the delay scan for a wavelength\n",
    "import json\n",
    "import fileParsingMethods\n",
    "import SeriesDegradation\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import sys \n",
    "sys.path.insert(1, r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\")\n",
    "import ExponentialFunctions\n",
    "#493 nm\n",
    "#saturation file\n",
    "data_file = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\STD-493\\saturation_2024-01-19_14-24.mat\"\n",
    "#fit file\n",
    "fitJSON = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\STD-493\\TAfitPump653Probe493_DecayCorrected.JSON\"\n",
    "pRatio = 1502/1575\n",
    "dOD, delay, _ = fileParsingMethods.parseSummaryFileToArray(data_file, absorbance=True, backgroundLen=25)\n",
    "print(np.shape(dOD))\n",
    "times = fileParsingMethods.parseTime(fileParsingMethods.getTimes(data_file))\n",
    "correctionFactors = SeriesDegradation.degradationCompensation(14100, times, decaysteps=len(delay))\n",
    "\n",
    "fitData = json.load(open(fitJSON))[\"entries\"][0]\n",
    "t_data = np.linspace(delay[0], delay[-1], 1000)\n",
    "fitData = fitData['startParameters']\n",
    "position = fitData[0]['position']\n",
    "slope = fitData[0]['slope']\n",
    "amplitudes = []\n",
    "decays = []\n",
    "for i in range(len(fitData)-1):\n",
    "    amplitudes.append(fitData[i+1]['amplitude'])\n",
    "    decays.append(fitData[i+1]['decay'])\n",
    "amplitudes = np.array(amplitudes)\n",
    "decays = np.array(decays)\n",
    "y_data = ExponentialFunctions.universalExponentialEnumerable(t_data, position, slope, amplitudes, decays)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(1,2.5), dpi=288)\n",
    "#ax.plot(delay*1e-3, np.mean(dOD, axis=0))\n",
    "print(np.shape(delay))\n",
    "ax.plot(delay*1e-3, np.mean(dOD*correctionFactors, axis=0), label = \"data\")\n",
    "ax.plot(t_data*1e-3,y_data, linestyle=\"--\", label=\"fit\")\n",
    "ax.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "ax.set_xlabel(r't / ps')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like example fit, but comparison of 440 orthogonal and parallel\n",
    "import json\n",
    "import fileParsingMethods\n",
    "import SeriesDegradation\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import sys \n",
    "sys.path.insert(1, r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\")\n",
    "import ExponentialFunctions\n",
    "#440 nm\n",
    "#parallel\n",
    "#saturation file\n",
    "data_fileParallel = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-440\\saturation_2024-01-16_19-07.mat\"\n",
    "#fit file\n",
    "fitJSON_Parallel = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-440\\TAfitPump653Probe440_DecayCorrected.JSON\"\n",
    "pRatioParallel = 1489/1575\n",
    "dOD_parallel, delay_parallel, _ = fileParsingMethods.parseSummaryFileToArray(data_fileParallel, absorbance=True, backgroundLen=30)\n",
    "timesParallel = fileParsingMethods.parseTime(fileParsingMethods.getTimes(data_fileParallel))\n",
    "correctionFactorsParallel = SeriesDegradation.degradationCompensation(12600, timesParallel, decaysteps=len(delay_parallel), powerDensities=pRatioParallel)\n",
    "\n",
    "fitDataPar = json.load(open(fitJSON_Parallel))[\"entries\"][0]\n",
    "t_dataPar = np.linspace(delay_parallel[0], delay_parallel[-1], 1000)\n",
    "fitDataPar = fitDataPar['startParameters']\n",
    "positionPar = fitDataPar[0]['position']\n",
    "slopePar = fitDataPar[0]['slope']\n",
    "amplitudesPar = []\n",
    "decaysPar = []\n",
    "for i in range(len(fitDataPar)-1):\n",
    "    amplitudesPar.append(fitDataPar[i+1]['amplitude'])\n",
    "    decaysPar.append(fitDataPar[i+1]['decay'])\n",
    "amplitudesPar = np.array(amplitudesPar)\n",
    "decaysPar = np.array(decaysPar)\n",
    "\n",
    "#orthogonal\n",
    "#saturation file\n",
    "data_fileOrth = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-440\\Orthogonal\\saturation_2024-01-16_16-59.mat\"\n",
    "#fit file\n",
    "fitJSON_Orth = r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\\SHG-440\\Orthogonal\\TAfitPump653Probe440OrthPol_DecayCorrected.JSON\"\n",
    "pRatioOrth = 1523/1575\n",
    "dOD_orth, delay_orth, _ = fileParsingMethods.parseSummaryFileToArray(data_fileOrth, absorbance=True, backgroundLen=30)\n",
    "timesOrth = fileParsingMethods.parseTime(fileParsingMethods.getTimes(data_fileOrth))\n",
    "correctionFactorsOrth = SeriesDegradation.degradationCompensation(12600, timesOrth, decaysteps=len(delay_orth), powerDensities=pRatioOrth)\n",
    "\n",
    "fitDataOrth = json.load(open(fitJSON_Orth))[\"entries\"][0]\n",
    "t_dataOrth = np.linspace(delay_orth[0], delay_orth[-1], 1000)\n",
    "fitDataOrth = fitDataOrth['startParameters']\n",
    "positionOrth = fitDataOrth[0]['position']\n",
    "slopeOrth = fitDataOrth[0]['slope']\n",
    "amplitudesOrth = []\n",
    "decaysOrth = []\n",
    "for i in range(len(fitDataOrth)-1):\n",
    "    amplitudesOrth.append(fitDataOrth[i+1]['amplitude'])\n",
    "    decaysOrth.append(fitDataOrth[i+1]['decay'])\n",
    "amplitudesOrth = np.array(amplitudesOrth)\n",
    "decaysOrth = np.array(decaysOrth)\n",
    "#y_data = ExponentialFunctions.universalExponentialEnumerable(t_data, position, slope, amplitudes, decays)\n",
    "\n",
    "\n",
    "\n",
    "#let me try to do the perrin formula\n",
    "r_anisotropy = lambda par, orth: (par-orth)/(par+2*orth)\n",
    "\n",
    "#stitch together parallel and orth to be on the same delay setting\n",
    "#taken from raw data plot above\n",
    "#create my own average time delay with n steps per picosecond and calculate the average value for plotting\n",
    "n_subticks = int(1)\n",
    "plotDelay = np.linspace(-2e4, 4e4, 30*n_subticks)\n",
    "plotData = np.zeros((2, len(plotDelay)))\n",
    "\n",
    "correctedParallel = np.mean(dOD_parallel*correctionFactorsParallel, axis=0)\n",
    "correctedOrth = np.mean(dOD_orth*correctionFactorsOrth, axis=0)\n",
    "\n",
    "for indDelay in range(len(plotDelay)):\n",
    "    delay_boolArray = (delay_orth[:]-positionOrth > plotDelay[indDelay]) & (delay_orth[:]-positionOrth < plotDelay[indDelay] + 1e3/(n_subticks))\n",
    "    plotData[0,indDelay] = np.mean(correctedOrth[delay_boolArray], axis = None)\n",
    "for indDelay in range(len(plotDelay)):\n",
    "    delay_boolArray = (delay_parallel[:]-positionPar > plotDelay[indDelay]) & (delay_parallel[:]-positionPar < plotDelay[indDelay] + 1e3/(n_subticks))\n",
    "    plotData[1,indDelay] = np.mean(correctedParallel[delay_boolArray], axis = None)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2,2.5), dpi=288)\n",
    "#ax.plot(delay*1e-3, np.mean(dOD, axis=0))\n",
    "#ax.plot(delay_parallel*1e-3-positionPar*1e-3, correctedParallel, color = \"r\", label=\"parallel\")\n",
    "ax.plot(plotDelay*1e-3, plotData[1], color=\"r\", linestyle=\"dotted\", label=\"parallel\")\n",
    "#ax.plot(delay_orth*1e-3-positionOrth*1e-3, correctedOrth*2, color = \"b\", label=r\"2 $\\times$ orthogonal\")\n",
    "ax.plot(plotDelay*1e-3, plotData[0]*2, color = \"b\", linestyle=\"dotted\", label=r\"2 $\\times$ orthogonal\")\n",
    "ax.set_xlim((-25, 40))\n",
    "#ax.plot(t_data*1e-3,y_data, linestyle=\"--\")\n",
    "ax.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "ax.set_xlabel(r'delay / ps')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2,2.5), dpi=288)\n",
    "#plt.plot(plotDelay, plotData[0,:])\n",
    "#plt.plot(plotDelay, plotData[1,:])\n",
    "ax2.plot(plotDelay*1e-3, r_anisotropy(plotData[1], plotData[0]))\n",
    "ax2.set_ylim((0,0.5))\n",
    "ax2.set_xlim((0,30))\n",
    "ax2.set_ylabel('r anisotropy / 1')\n",
    "ax2.set_xlabel('delay / ps')\n",
    "ax2.set_yticks([0, 0.25, 0.4, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot delay scans of 493 and 680 nm\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "import sys\n",
    "sys.path.insert(1, mainDataDirectoryPath)\n",
    "from os.path import join\n",
    "import ShowDelayScan\n",
    "#493 nm\n",
    "ShowDelayScan.fromFiles(join(mainDataDirectoryPath, r\"STD-493\\saturation_2024-01-19_14-24.mat\"), fig_size = plotHelperLatex.figSizer(1,4), showMean=False)\n",
    "#680 nm\n",
    "ShowDelayScan.fromFiles(join(mainDataDirectoryPath, r\"STD-680\\saturation_2024-01-16_14-09.mat\"), fig_size = plotHelperLatex.figSizer(1,4), showMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Radiance variation test\n",
    "Plotting maximum signal response at a wavelength (680 nm) against pump and probe powers, where one of the powers is kept fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "import json\n",
    "from fileParsingMethods import numberFileGenerator\n",
    "from os.path import join\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "#From 2023.12.22 PumpPowerVar in pump 653 Probe 680\n",
    "#2nd-x;170.98460380387596;4.882207633023595;1.0;0.0;3.5228\n",
    "#2nd-x Error;0.044586461622659335;0.10565856496080137;60.88881374774401;1.876611874015875;3.5228\n",
    "#2nd-y;123.76193757659699;5.398671742339542;1.0;0.0;3.5228\n",
    "#2nd-y Error;0.007988920725724555;0.018944810621290376;10.67173147785706;0.31277821947196766;3.5228\n",
    "\n",
    "file = open(join(mainDataDirectoryPath, r\"STD-680\\2023.12.22_PumpPowerVar\\TAfitPump653Probe680_OUTPUT.JSON\"))\n",
    "entries = json.load(file)['entries']\n",
    "entries_files = []\n",
    "dOD = []\n",
    "for entry in entries:\n",
    "    entries_files.append(entry['inputFile'])\n",
    "    dOD.append([entry['popt'][2], np.sqrt(entry['pcov'][2][2])])\n",
    "\n",
    "dOD = np.array(dOD)\n",
    "dOD[dOD == np.inf] = np.NaN\n",
    "#print(dOD)\n",
    "#these are in order\n",
    "pumpRad = np.array([[610, 45], [610, 45], [610, 45], [3195, 142], [4357,172], [5810, 269], [4358, 230], [4358, 230], [4358, 230]])\n",
    "probeRad = np.array([[916, 100], [1831, 106], [458, 50], [458, 50], [458, 50], [458, 50], [916, 53], [3663, 74], [1831, 60]])\n",
    "\n",
    "\n",
    "#try calculating a fun correction factor based on the first 3 measurements that have identical pump radiance\n",
    "#need to take the times such as done in degradation\n",
    "io.loadmat(join(mainDataDirectoryPath, r\"STD-680\\2023.12.22_PumpPowerVar\\saturation_2023-12-22_11-47.mat\"))\n",
    "#print(np.polyfit(pumpRad[2:6,0], dOD[2:6,0], 1, cov=True))\n",
    "linParameter = np.polyfit(pumpRad[2:5,0], dOD[2:5,0], 1)\n",
    "#print(pumpRad[])\n",
    "#Plot first 6 points as they should have relatively little sample degradation, because of their order and the pump power\n",
    "fig, ax1 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2,2.6/2), dpi = 288)\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2,2.6/2), dpi = 288)\n",
    "#plt.tight_layout()\n",
    "#ax1.plot(pumpRad[:6,0], dOD[:6,0], 'b.')\n",
    "#yerror of dOD fitting is not sensible\n",
    "ax1.plot(pumpRad[(0,5),0]/2e5, np.polyval(linParameter, pumpRad[(0,5),0]), 'royalblue', linestyle=\"dashed\")\n",
    "ax1.errorbar(pumpRad[:6,0]/2e5, dOD[:6,0], xerr = pumpRad[:6,1]/2e5, ls = \"None\", ecolor = 'b', capsize = 2)\n",
    "\n",
    "ax1.set_xlabel(r'pump radiant exposure $H_{x_0,y_0}$ / $\\mathrm{\\frac{J}{m^2}}$')\n",
    "ax1.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "ax1.set_ylim([0,-2.2])\n",
    "#ax2.plot(probeRad[6:,0], dOD[6:,0], 'b')\n",
    "ax2.errorbar(probeRad[6:,0]/4e5, dOD[6:,0], xerr = probeRad[6:,1]/4e5, yerr=abs(dOD[6:,0]*pumpRad[6:,1]/pumpRad[6:,0]), ls=\"None\", ecolor = \"b\", capsize = 2)\n",
    "ax2.set_xlabel(r'probe radiant exposure $H_{x_0,y_0}$ / $\\mathrm{\\frac{J}{m^2}}$')\n",
    "ax2.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "\n",
    "\n",
    "#including degradation\n",
    "import SeriesDegradation\n",
    "dirPath = join(mainDataDirectoryPath, r\"STD-680\\2023.12.22_PumpPowerVar\")\n",
    "filePaths = []\n",
    "start = 6615\n",
    "stop = 6620\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "start = 6630\n",
    "stop = 6632\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "start = 6636\n",
    "stop = 6644\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "\n",
    "p_ratio = 590/1575\n",
    "powerDensitiesIn = [1.05 for i in range(6)]\n",
    "powerDensitiesIn = np.concatenate((powerDensitiesIn, [5.5 for i in range(3)][:]), axis = None)\n",
    "powerDensitiesIn = np.concatenate((powerDensitiesIn, [7.5 for i in range(3)][:]), axis = None)\n",
    "powerDensitiesIn = np.concatenate((powerDensitiesIn, [10 for i in range(3)][:]), axis = None)\n",
    "powerDensitiesIn = np.array(powerDensitiesIn)*p_ratio\n",
    "timesPump = SeriesDegradation.getTimes(filePaths, dirPath)\n",
    "\n",
    "\n",
    "#get delta t of times\n",
    "timesFromZeroPump = SeriesDegradation.parseTime(timesPump)\n",
    "CorrectionEstimatedPump = SeriesDegradation.degradationCompensation(14100, timesFromZeroPump, 1, powerDensities=powerDensitiesIn)\n",
    "\n",
    "corr_dODpumpVar = dOD[:6,0][:]*CorrectionEstimatedPump[1:-1:3].T\n",
    "corr_dODpumpVar = corr_dODpumpVar[0]\n",
    "ax1.plot(pumpRad[:6,0]/2e5, corr_dODpumpVar, \"r.\", linestyle=\"None\")\n",
    "pumpVar_corrFit = np.polyfit(pumpRad[:6,0], corr_dODpumpVar, deg = 2)\n",
    "ax1.plot(pumpRad[[0,5],0]/2e5, np.polyval(pumpVar_corrFit, pumpRad[[0,5],0]), \"orange\", linestyle=\"dashed\")\n",
    "### Probe power var\n",
    "filePaths = []\n",
    "start = 6645\n",
    "stop = 6653\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "\n",
    "timesProbe = SeriesDegradation.getTimes(filePaths, dirPath)\n",
    "#get delta t of times\n",
    "timesFromZeroProbe = SeriesDegradation.parseTime(timesProbe)\n",
    "CorrectionEstimatedProbe = SeriesDegradation.degradationCompensation(12600, timesFromZeroProbe, 1, powerDensities=7.5*p_ratio)\n",
    "corr_dODprobeVar = dOD[6:,0][:]*CorrectionEstimatedProbe[1:-1:3].T\n",
    "corr_dODprobeVar = corr_dODprobeVar[0]\n",
    "#ax2.plot(probeRad[6:,0], corr_dODprobeVar, 'r.', linestyle=\"None\")\n",
    "ax2.errorbar(probeRad[6:,0]/4e5, corr_dODprobeVar, yerr = abs(corr_dODprobeVar*pumpRad[6:,1]/pumpRad[6:,0]), xerr = probeRad[6:,1]/4e5, linestyle=\"None\", capsize=2, ecolor=\"r\")\n",
    "ticks = [2e-3,  4e-3, 6e-3, 8e-3, 10e-3]\n",
    "ax2.set_xticks(ticks, [\"2e-3\", \"4e-3\", \"6e-3\", \"8e-3\", \"10e-3\"])\n",
    "#fig.suptitle('no degradation correction yet')\n",
    "\n",
    "###########################custom legend###############################\n",
    "from matplotlib.lines import Line2D\n",
    "legend_items = [Line2D([0], [0], color = \"blue\", label=\"raw data\", marker=\"s\", linestyle=\"None\"),\n",
    "                Line2D([0], [0], color = \"red\", label=\"corrected\", marker=\"s\", linestyle=\"None\"),\n",
    "                Line2D([0], [0], color = \"black\", label=\"linear fit\", marker=\"None\",linestyle=\"dashed\")]\n",
    "#for ind in range(len(times)):\n",
    "#    legend_items.append(Line2D([0], [0], color = colors[ind], label=\"%.1f ps\" %(times[ind]*1e-3), marker=\"s\", linestyle=\"None\"))\n",
    "\n",
    "ax1.legend(handles=legend_items)\n",
    "ax2.legend(handles=legend_items[:2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot traces of different\n",
    "I don't think I ever made this work properly? I mean the background alignment. Need to think if there is a way to easily adjust fromFiles or rather the normal background subtraction function \\\n",
    "I am not sure this is really needed though, this is an odd usecase I have to check if I ever even put into the thesis. Reason is it is averaging the background over all different measurements here. Split them/use a different function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "####################FIX PATH TO PHDMATLAB##################################\n",
    "#sys.path.insert(1, r\"C:\\Users\\M\\Documents\\phdmatlab\\sqib-pmma-probe-wavelength\\UV_Setup\\new_parallel_pol_pump653nm\")\n",
    "sys.path.insert(1, mainDataDirectoryPath)\n",
    "from os.path import join\n",
    "from ShowDelayScan import fromFiles\n",
    "\n",
    "#this is the constant pump power and probe power variation\n",
    "filenames=[]\n",
    "for x in range(6645,6654,1):\n",
    "    filenames.append(r\"\\TA_fourier_%4d\" %(x))\n",
    "##################PATH AGAIN#####################\n",
    "dirPath = join(mainDataDirectoryPath, r\"STD-680\\2023.12.22_PumpPowerVar\")\n",
    "#fromFiles is not malfunctioning, but exactly doing what it is supposed to\n",
    "#the problem is I am feeding them all in together so it does a background subtraction instead of individual\n",
    "fromFiles(filenames, dirPath, plotHelperLatex.figSizer(2), False)\n",
    "\n",
    "filenames=[]\n",
    "for x in range(6615,6621,1):\n",
    "    filenames.append(r\"\\TA_fourier_%4d\" %(x))\n",
    "for x in range(6630,6633,1):\n",
    "    filenames.append(r\"\\TA_fourier_%4d\" %(x))\n",
    "fromFiles(filenames, dirPath, plotHelperLatex.figSizer(2), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "from scipy import io \n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "import SeriesDegradation\n",
    "\n",
    "    \n",
    "\n",
    "testArray = [join(mainDataDirectoryPath , r\"Pump653Probe493_Degradation\\TA_fourier_6778.mat\"), \n",
    "             join(mainDataDirectoryPath , r\"Pump653Probe493_Degradation\\TA_fourier_6779.mat\")]\n",
    "\n",
    "#to be tested yet\n",
    "dArray, delay, _ = SeriesDegradation.parseSummaryFileToArray(join(mainDataDirectoryPath, r\"Pump653Probe493_Degradation\\saturation_2024-01-19_16-14.mat\"))\n",
    "\n",
    "\n",
    "#powerVar = pd.read_excel(\"CorrectedPump653ProbeWavelengthScan_UV-extended.xlsx\", sheet_name='Degradation493nm')\n",
    "dtime = SeriesDegradation.getTimes(join(mainDataDirectoryPath , r\"Pump653Probe493_Degradation\\saturation_2024-01-19_16-14.mat\"))\n",
    "dtime = SeriesDegradation.parseTime(dtime)\n",
    "#dOD = powerVar['dOD / mOD']\n",
    "\n",
    "dArray, aArray, _ = SeriesDegradation.removeBackground(dArray, 20)\n",
    "\n",
    "t0_estimate = np.argmax((aArray[0,1:]-aArray[0,:-1])/(delay[1:]-delay[:-1]))\n",
    "print(t0_estimate)\n",
    "popt, pcov = SeriesDegradation.fitDegradation(aArray, dtime, t0=t0_estimate)\n",
    "\n",
    "print(popt)\n",
    "print(np.sqrt(pcov))\n",
    "\n",
    "\n",
    "timeAt = np.array([0,1e3,5e3]) #fs\n",
    "\n",
    "\n",
    "Corr = SeriesDegradation.degradationCompensation(popt, dtime, np.shape(aArray)[1])\n",
    "fig, ax = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(1,3), dpi = 288)\n",
    "for i in range(len(timeAt)):\n",
    "    ax.plot(dtime, aArray[:,i], label=\"delay = %.1f ps\" %(timeAt[i]*1e-3))\n",
    "    ax.plot(dtime, aArray[:,i]*Corr[:,i])\n",
    "#ax.plot(dtime, dOD[0,2]-dtime*popt)\n",
    "ax.legend()\n",
    "ax.set_xlabel('time / s')\n",
    "ax.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degradation over time with different delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SeriesDegradation\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "from os.path import join\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "\n",
    "def numberFileGenerator(start, stop, namePrefix=r\"TA_fourier_\", nameSuffix=r\"\"):\n",
    "    ''''generate TA_fourier_02364 style names in list form'''\n",
    "    vec = np.arange(start, stop+1,1)\n",
    "    filenames = []\n",
    "    for number in vec:\n",
    "        filenames.append(namePrefix + str(number)+ nameSuffix)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "start = 6778\n",
    "stop = 6787\n",
    "dirPath = join(mainDataDirectoryPath , r\"Pump653Probe493_Degradation\")\n",
    "\n",
    "\n",
    "SeriesDegradation.plotTrend(numberFileGenerator(start, stop), dirPath, plotHelperLatex.figSizer(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SeriesDegradation\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "import fileParsingMethods\n",
    "import sys\n",
    "sys.path.insert(1, mainDataDirectoryPath)\n",
    "import ShowDelayScan\n",
    "\n",
    "#need to switch getTimes to the same system as parseSummary to allow any type of input\n",
    "if True:\n",
    "    start = 6778\n",
    "    stop = 6787\n",
    "    dirPath = sys.path.join(mainDataDirectoryPath,r\"Pump653Probe493_Degradation\")\n",
    "else:\n",
    "    start = 6645\n",
    "    stop = 6653\n",
    "    dirPath = sys.path.join(mainDataDirectoryPath , r\"STD-680\\2023.12.22_PumpPowerVar\")\n",
    "\n",
    "\n",
    "filePaths = fileParsingMethods.numberFileGenerator(start, stop)\n",
    "print(len(filePaths))\n",
    "times = SeriesDegradation.getTimes(filePaths, dirPath)\n",
    "dArray, delay,_ = SeriesDegradation.parseSummaryFileToArray(filePaths, dirPath)\n",
    "_, aArray, _ = SeriesDegradation.removeBackground(dArray, 20)\n",
    "#get delta t of times\n",
    "timesFromZero = np.array(SeriesDegradation.parseTime(times),dtype=int)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2.5), dpi = 288)\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2.5), dpi = 288)\n",
    "for ind in range(np.shape(aArray)[0]):\n",
    "    ax2.plot(delay*1e-3, aArray[ind])\n",
    "\n",
    "#estimate t0 by finding maximum value in aArray\n",
    "t0_estimate = np.argmax((aArray[0,1:]-aArray[0,:-1])/(delay[1:]-delay[:-1]))\n",
    "print(t0_estimate)\n",
    "popt, pcov = SeriesDegradation.fitDegradation(aArray, timesFromZero, t0 = t0_estimate+2, sliding_window_len=5)\n",
    "print(popt)\n",
    "print(np.sqrt(pcov))\n",
    "\n",
    "Correction = SeriesDegradation.degradationCompensation(popt, timesFromZero, np.shape(aArray)[1], powerDensities=1)\n",
    "#Correction = SeriesDegradation.degradationCompensation(14000, timesFromZero, np.shape(aArray)[1], powerDensities=2.8)\n",
    "CorrArray = np.zeros(np.shape(aArray))\n",
    "for ind in range(np.shape(aArray)[0]):\n",
    "    CorrArray[ind,:] = aArray[ind,:]*Correction[ind,:]\n",
    "print(np.shape(aArray))\n",
    "#timesFromZero = np.append(timesFromZero, np.mean(timesFromZero[1:]-timesFromZero[:-1], axis = None))\n",
    "delays, measurementTimes = np.meshgrid(delay, timesFromZero)\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,4), dpi = 288)\n",
    "map = ax.pcolor(delays*1e-3, measurementTimes, CorrArray, cmap=\"plasma\", snap=False)\n",
    "ax.set_xlabel('delay time / ps')\n",
    "ax.set_ylabel('total time elapsed at start / s')\n",
    "fig.colorbar(map, ax=ax, label=r\"$\\Delta A$ / a.u.\")\n",
    "ax.set_yticks(timesFromZero-np.mean(timesFromZero[1]-timesFromZero[0], axis=None)/2, timesFromZero)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "for ind in range(np.shape(aArray)[0]):\n",
    "    ax3.plot(delay*1e-3, CorrArray[ind])\n",
    "#ax2.set_xlim([-5, 50])\n",
    "#ax3.set_xlim([-5, 50])\n",
    "#ax2.set_ylim([0.3,0.8])\n",
    "#ax3.set_ylim([0.3,0.8])\n",
    "ax2.set_xlabel('delay time / ps')\n",
    "ax2.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "ax3.set_xlabel('delay time / ps')\n",
    "ax3.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of powervariation \\\n",
    "I do not know why the alignment no longer works, nothing in the branch changes on git seem to indicate something that should have changed this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing a comparison of powervar here\n",
    "import SeriesDegradation\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "import ShowDelayScan\n",
    "from fileParsingMethods import numberFileGenerator\n",
    "from os.path import join\n",
    "\n",
    "start = 6645\n",
    "stop = 6653\n",
    "dirPath = join(mainDataDirectoryPath , r\"STD-680\\2023.12.22_PumpPowerVar\")\n",
    "\n",
    "\n",
    "filePaths = numberFileGenerator(start, stop)\n",
    "times = SeriesDegradation.getTimes(filePaths, dirPath)\n",
    "dArray, delay, _ = SeriesDegradation.parseSummaryFileToArray(filePaths, dirPath, False)\n",
    "_, aArray, _ = SeriesDegradation.removeBackground(dArray, 10)\n",
    "#get delta t of times\n",
    "timesFromZero = SeriesDegradation.parseTime(times)\n",
    "fig1, ax1 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(3.2), dpi = 288)\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(3.2), dpi = 288)\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(3.2), dpi = 288)\n",
    "for ind in range(np.shape(aArray)[0]):\n",
    "    ax2.plot(delay*1e-3, aArray[ind])\n",
    "\n",
    "#estimate t0 by finding maximum value in aArray\n",
    "t0_estimate = np.argmax((aArray[0,1:]-aArray[0,:-1])/(delay[1:]-delay[:-1]))\n",
    "print(t0_estimate)\n",
    "popt, pcov = SeriesDegradation.fitDegradation(aArray, timesFromZero, t0 = t0_estimate+5, sliding_window_len=5)\n",
    "print(popt)\n",
    "print(np.sqrt(pcov))\n",
    "refPowerDensity = 1575\n",
    "powDens680 = 581*7.5\n",
    "powDensRatio = powDens680/refPowerDensity\n",
    "\n",
    "CorrectionNative = SeriesDegradation.degradationCompensation(popt, timesFromZero, np.shape(aArray)[1], powerDensities=1)\n",
    "CorrectionEstimated = SeriesDegradation.degradationCompensation(12620, timesFromZero, np.shape(aArray)[1], powerDensities=powDensRatio)\n",
    "CorrArray = np.zeros(np.shape(aArray))\n",
    "for ind in range(np.shape(aArray)[0]):\n",
    "    ax1.plot(delay*1e-3, aArray[ind,:])\n",
    "    ax2.plot(delay*1e-3, aArray[ind,:]*CorrectionNative[ind,:])\n",
    "    ax3.plot(delay*1e-3, aArray[ind,:]*CorrectionEstimated[ind,:])\n",
    "\n",
    "\n",
    "#fig1.suptitle(\"raw\")\n",
    "#fig2.suptitle(\"Native\")\n",
    "#fig3.suptitle(\"Estimate\")\n",
    "ax1.set_xlabel('delay time / ps')\n",
    "ax1.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "ax2.set_xlabel('delay time / ps')\n",
    "ax2.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "ax3.set_xlabel('delay time / ps')\n",
    "ax3.set_ylabel(r'$\\Delta A$ / mOD')\n",
    "#ax1.set_ylim((-1.2,-0.8))\n",
    "#ax2.set_ylim((-1.2,-0.8))\n",
    "#ax3.set_ylim((-1.2,-0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing a comparison of powervar here\n",
    "import SeriesDegradation\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "import ShowDelayScan\n",
    "from fileParsingMethods import numberFileGenerator\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "dirPath = join(mainDataDirectoryPath , r\"STD-680\\2023.12.22_PumpPowerVar\")\n",
    "filePaths = []\n",
    "start = 6615\n",
    "stop = 6620\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "start = 6630\n",
    "stop = 6632\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "start = 6636\n",
    "stop = 6644\n",
    "filePaths += numberFileGenerator(start, stop)\n",
    "\n",
    "p_ratio = 590/1580\n",
    "powerDensitiesIn = [1.05 for i in range(6)]\n",
    "powerDensitiesIn = np.concatenate((powerDensitiesIn, [5.5 for i in range(3)][:]), axis = None)\n",
    "powerDensitiesIn = np.concatenate((powerDensitiesIn, [7.5 for i in range(3)][:]), axis = None)\n",
    "powerDensitiesIn = np.concatenate((powerDensitiesIn, [10 for i in range(3)][:]), axis = None)\n",
    "powerDensitiesIn = np.array(powerDensitiesIn)*p_ratio\n",
    "times = SeriesDegradation.getTimes(filePaths, dirPath)\n",
    "dArray, delay, _ = SeriesDegradation.parseSummaryFileToArray(filePaths, dirPath)\n",
    "_, aArray, _ = SeriesDegradation.removeBackground(dArray, 10)\n",
    "\n",
    "#get delta t of times\n",
    "timesFromZero = SeriesDegradation.parseTime(times)\n",
    "fig1, ax1 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(2), dpi = 288)\n",
    "#fig2, ax2 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(3.2), dpi = 144)\n",
    "#fig3, ax3 = plt.subplots(1,1, figsize=plotHelperLatex.figSizer(3.2), dpi = 144)\n",
    "\n",
    "\n",
    "#estimate t0 by finding maximum value in aArray\n",
    "t0_estimate = np.argmax((aArray[0,1:]-aArray[0,:-1])/(delay[1:]-delay[:-1]))\n",
    "print(t0_estimate)\n",
    "#popt, pcov = SeriesDegradation.fitDegradation(aArray, timesFromZero, t0 = t0_estimate+2, sliding_window_len=5)\n",
    "#print(popt)\n",
    "#print(np.sqrt(pcov))\n",
    "\n",
    "\n",
    "CorrectionEstimated = SeriesDegradation.degradationCompensation(14100, timesFromZero, np.shape(aArray)[1], powerDensities=powerDensitiesIn)\n",
    "#CorrectionEstimated = SeriesDegradation.degradationCompensation(14100, timesFromZero, np.shape(aArray)[1], powerDensities=2.8)\n",
    "CorrArray = np.zeros(np.shape(aArray))\n",
    "for ind in range(np.shape(aArray)[0]):\n",
    "    ax1.plot(delay*1e-3, aArray[ind,:]*CorrectionEstimated[ind,:])\n",
    "    #ax2.plot(delay*1e-3, aArray[ind,:]*CorrectionNative[ind,:])\n",
    "    #ax3.plot(delay*1e-3, aArray[ind,:]*CorrectionEstimated[ind,:])\n",
    "\n",
    "\n",
    "#fig1.suptitle(\"raw\")\n",
    "#fig2.suptitle(\"Native\")\n",
    "#fig3.suptitle(\"Estimate\")\n",
    "ax1.set_xlabel('delay time / ps')\n",
    "ax1.set_ylabel(r'$\\Delta$OD / mOD')\n",
    "#ax2.set_xlabel('delay time / ps')\n",
    "#ax2.set_ylabel('dOD / mOD')\n",
    "#ax3.set_xlabel('delay time / ps')\n",
    "#ax3.set_ylabel('dOD / mOD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for accuracy of overlap correction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import constants as const\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "from os.path import join\n",
    "import json\n",
    "sys.path.insert(1, mainDataDirectoryPath)\n",
    "from peakRadianceAuto import autoPeakRadiance\n",
    "from ArtrayAnalysis import ArtFit\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "from matplotlib import ticker\n",
    "\n",
    "def correctionFactorWithError(pathJSON, fittingPath):\n",
    "    '''Returns correction factor with uncertainty in dOD per W/m^2\\\\\n",
    "        pathJSON is to summary\\\\\n",
    "            fittingPath is to standard fit.txt (2 beam pointspreads supported)'''\n",
    "    #don't care about loading these multiple times\n",
    "    #returns first \n",
    "    peakRad, dPeakRad = autoPeakRadiance(pathJSON, fittingPath)\n",
    "    #only choose pump\n",
    "    peakRad = peakRad[1]\n",
    "    dPeakRad = dPeakRad[1]\n",
    "    #dict with {\"pump/probe\": {\"x/y\" : [[sig, error],...]}}\n",
    "    dataDict = ArtFit.parseFitFile(fittingPath)\n",
    "\n",
    "    pump = dataDict[\"pump\"]\n",
    "    probe = dataDict[\"probe\"]\n",
    "    #coeffs = x_center, sig, Amplitude, offset\n",
    "    #sig1Error = lambda corr, sig1, sig2, muDiff, dsig1 : dsig1*corr*(sig1**3/sig2**2+sig1+muDiff**2*sig1/(sig1**2+sig2**2)**2)\n",
    "    #sig2Error = lambda corr, sig1, sig2, muDiff, dsig2: dsig2*corr*((sig1**2+sig2**2)/sig2 + muDiff**2*sig2/(sig1**2+sig2**2)**2)\n",
    "    #muDiffError = lambda corr, sig1, sig2, muDiff, dmuDiff: dmuDiff*corr*muDiff/(sig1**2+sig2**2)\n",
    "\n",
    "    sig1Error = lambda corr, coeffs1, coeffs2: coeffs1[1,1]*corr*(coeffs1[1,0]**3/coeffs2[1,0]**2+coeffs1[1,0]+(coeffs1[0,0]-coeffs2[0,0])**2*coeffs1[1,0]/(coeffs1[1,0]**2+coeffs2[1,0]**2)**2)\n",
    "    sig2Error = lambda corr, coeffs1, coeffs2: coeffs2[1,1]*corr*((coeffs1[1,0]**2+coeffs2[1,0]**2)/coeffs2[1,0]+ (coeffs1[0,0]-coeffs2[0,0])**2*coeffs2[1,0]/(coeffs1[1,0]**2+coeffs2[1,0]**2)**2)\n",
    "    #using compound error for this one\n",
    "    muDiffError = lambda corr, coeffs1, coeffs2: (coeffs1[0,1]+coeffs2[0,1])*corr*(coeffs1[0,0]-coeffs2[0,0])/(coeffs1[1,0]**2+coeffs2[1,0]**2)\n",
    "    #simplification\n",
    "    fullError = lambda corr, coeffs1, coeffs2: sig1Error(corr, coeffs1, coeffs2) + sig2Error(corr, coeffs1, coeffs2) + muDiffError(corr, coeffs1, coeffs2)\n",
    "    #takes pump then probe\n",
    "    xCorr = ArtFit.calculateOverlapCorrection(pump[\"x\"][:,0], probe[\"x\"][:,0], 5)\n",
    "    dXCorr = fullError(xCorr, pump['x'], probe['x'])\n",
    "    yCorr = ArtFit.calculateOverlapCorrection(pump[\"y\"][:,0], probe[\"y\"][:,0], 5)\n",
    "    dYCorr = fullError(xCorr, pump['y'], probe['y'])\n",
    "    Corr = [xCorr, yCorr]\n",
    "    totalCorr = xCorr*yCorr\n",
    "    totalError = (xCorr*dYCorr+yCorr*dXCorr)/peakRad + totalCorr*dPeakRad/peakRad**2\n",
    "    #print(totalCorr)\n",
    "    #print(totalError)\n",
    "    sigCalc = lambda probeCoeff, pumpCoeff: [probeCoeff[1,0]/pumpCoeff[1,0], probeCoeff[1,1]/pumpCoeff[1,0] + probeCoeff[1,0]/pumpCoeff[1,0]**2*pumpCoeff[1,1]]\n",
    "    sigbysig = np.array([sigCalc(probe['x'], pump['x']), sigCalc(probe['y'], pump['y'])], dtype = float)\n",
    "    dmu = np.array([[abs(probe[\"x\"][0,0]-pump[\"x\"][0,0]), probe[\"x\"][0,1]+pump[\"x\"][0,1]],[abs(probe[\"y\"][0,0]-pump[\"y\"][0,0]), probe[\"y\"][0,1]+pump[\"y\"][0,1]]], dtype = float)\n",
    "    return [Corr, totalError], sigbysig, dmu, pump, probe, peakRad\n",
    "\n",
    "def getNumericCorrection(summaryJSON):\n",
    "    f_JSON = open(summaryJSON, 'r')\n",
    "    summary_JSON = json.load(f_JSON)\n",
    "    f_JSON.close()\n",
    "    Routine = ArtFit(summary_JSON[\"BackgroundJSON\"], summary_JSON[\"pathProbe\"], summary_JSON[\"pathPump\"], summary_JSON[\"FitAvgWidth / px\"])\n",
    "    Routine.setPumpPower(summary_JSON[\"Pump power: W\"])\n",
    "    #Routine.setScale(summary_JSON[\"scale: Âµm/px\"])\n",
    "\n",
    "    Routine.plainFit(1)\n",
    "    Routine.plainFit(2)\n",
    "\n",
    "    numCorrection = Routine.getNumericCorrection(interpDensity = 32)\n",
    "    \n",
    "    return numCorrection\n",
    "\n",
    "\n",
    "\n",
    "file = open(join(mainDataDirectoryPath, r\"Pump653Probe493_ArtrayCompensationControl\\TAfitPump653Probe493_OUTPUT.JSON\"))\n",
    "entries = json.load(file)['entries']\n",
    "entries_files = []\n",
    "dOD = []\n",
    "for entry in entries:\n",
    "    entries_files.append(entry['inputFile'])\n",
    "    dOD.append([entry['popt'][2]+ entry['popt'][4], entry['pcov'][2][2]+entry['pcov'][4][2]])\n",
    "\n",
    "dOD = np.array(dOD)\n",
    "dOD[dOD == np.inf] = np.NaN\n",
    "\n",
    "\n",
    "\n",
    "basepath = lambda filename: join(mainDataDirectoryPath, r\"Pump653Probe493_ArtrayCompensationControl\" ,filename)\n",
    "inputFiles = [(r\"\\SummaryPump653Probe493Caution.JSON\", r\"\\BeamFitsPump653Probe493Caution.txt\"),\n",
    "              (r\"\\SummaryPump653Probe493_ShiftedProbe.JSON\", r\"\\BeamFitsPump653Probe493_ShiftedProbe.txt\"),\n",
    "              (r\"\\SummaryPump653Probe493_WideProbe.JSON\", r\"\\BeamFitsPump653Probe493_WideProbe.txt\"),\n",
    "              (r\"\\SummaryPump653Probe493_ThinProbe.JSON\", r\"\\BeamFitsPump653Probe493_ThinProbe.txt\")]\n",
    "labels = [\"standard\", \"shifted\", \"wide probe\", \"thin probe\"]\n",
    "\n",
    "correctionFactors = []\n",
    "numericCorrectionFactors = []\n",
    "dmu = np.zeros((4,2,2))\n",
    "sigbysig = np.zeros((4,2,2))\n",
    "pump_Coeffs =[]\n",
    "probe_Coeffs = []\n",
    "\n",
    "\n",
    "\n",
    "for index, shortPath in enumerate(inputFiles):\n",
    "    tempCorrectionFactors, sigbysig[index], dmu[index], pump_temp, probe_temp, peakRad = correctionFactorWithError(basepath(shortPath[0]), basepath(shortPath[1]))\n",
    "    correctionFactors.append(tempCorrectionFactors)\n",
    "    #numericCorrectionFactors.append(getNumericCorrection(basepath(shortPath[0])))\n",
    "    print(\"xsigRatio: %.3e +- %.3e, xdmu: %.3e +- %.3e\" %(sigbysig[index, 0,0], sigbysig[index, 0,1], dmu[index, 0,0], dmu[index, 0,1]))\n",
    "    print(\"ysigRatio: %.3e +- %.3e, ydmu: %.3e +- %.3e\" %(sigbysig[index,1,0], sigbysig[index,1,1], dmu[index,1,0], dmu[index,1,1]))\n",
    "    print(\"%s: %.3e +- %.3e dOD per W/msquare\" %(labels[index], correctionFactors[index][0][0]*correctionFactors[index][0][1]*dOD[index, 0]/peakRad, correctionFactors[index][1]*dOD[index, 0]))\n",
    "    pump_Coeffs.append(pump_temp)\n",
    "    probe_Coeffs.append(probe_temp)\n",
    "#plotting\n",
    "fig, axs = plt.subplots(3,4, figsize = plotHelperLatex.figSizer(0.9,2.5), dpi = 288)\n",
    "\n",
    "#doing all 4 measurements horizontally\n",
    "def contourPlot(axis_handle, coeffs1, coeffs2, x, y, lvl = 5, colormap =\"viridis\"):\n",
    "    #x, y are x and y vectors to be used\n",
    "    \"coeffs = center, sig, Amplitude, offset\"\n",
    "    [X_contour, Y_contour] = np.meshgrid(x, y)\n",
    "    Z_contour = ArtFit.gaussian(X_contour, coeffs1)*ArtFit.gaussian(Y_contour, coeffs2)\n",
    "\n",
    "    #adjust for correct height\n",
    "    Z_contour = Z_contour/np.sqrt(coeffs1[2]*coeffs2[2])\n",
    "    axis_handle.contour(X_contour, Y_contour, Z_contour, levels = lvl, cmap = colormap)\n",
    "    axis_handle.set_aspect('equal')\n",
    "\n",
    "ticker.MaxNLocator(integer = True)\n",
    "\n",
    "def textGenerator(axis_handle, sig, dmu, correctionFactors):\n",
    "    step= -0.15*3\n",
    "    axis_handle.text(0,0, r\"x: $\\Delta \\mu$=%.1f px\" %dmu[0,0])\n",
    "    axis_handle.text(0,step, r\"$\\frac{\\sigma_{probe}}{\\sigma_{pump}}$=%.2f\" %sig[0,0])\n",
    "    axis_handle.text(0,2*step, r\"Correction: %.2f\" %correctionFactors[0])\n",
    "    axis_handle.text(0,3*step, r\"y: $\\Delta \\mu$=%.1f px\" %dmu[1,0])\n",
    "    axis_handle.text(0,4*step, r\"$\\frac{\\sigma_{probe}}{\\sigma_{pump}}$=%.2f\" %sig[1,0])\n",
    "    axis_handle.text(0,5*step, r\"Correction: %.2f\" %correctionFactors[1])\n",
    "    \n",
    "\n",
    "\n",
    "#do legend on top\n",
    "axs[0,0].text(0.5,0, \"pump pointspread\", verticalalignment =\"center\")\n",
    "axs[0,0].plot(0,0, marker = 's', color = \"purple\")\n",
    "axs[0,0].text(0.5,0.05, \"  probe pointspread\", verticalalignment =\"center\")\n",
    "axs[0,0].plot(0,0.05, marker = 's', color = \"green\")\n",
    "axs[0,0].set_xlim((-0.5,4.5))\n",
    "axs[0,0].set_ylim((-0.05,0.1))\n",
    "\n",
    "for index, shortPath in enumerate(inputFiles):\n",
    "    #fix the y inversion of the old fits\n",
    "    pump_Coeffs[index]['y'][0,0] = 300-pump_Coeffs[index]['y'][0,0]\n",
    "    probe_Coeffs[index]['y'][0,0] = 300-probe_Coeffs[index]['y'][0,0]\n",
    "\n",
    "    #remove legend axis\n",
    "    axs[0,index].set_axis_off()\n",
    "    axs[2,index].set_axis_off()\n",
    "    axs[0,index].set_adjustable('box')\n",
    "    #find x and y vectors to be used\n",
    "    x = (pump_Coeffs[index]['x'][0,0] + probe_Coeffs[index]['x'][0,0])/2\n",
    "    y = (pump_Coeffs[index]['y'][0,0] + probe_Coeffs[index]['y'][0,0])/2\n",
    "    sigMax = max([pump_Coeffs[index]['x'][1,0], pump_Coeffs[index]['y'][1,0], probe_Coeffs[index]['x'][1,0], probe_Coeffs[index]['y'][1,0]])\n",
    "    x = np.linspace(x-2*sigMax, x+2*sigMax, 100)\n",
    "    #300 for inversion since, old fit files with vertical mirror in artrayanalysis\n",
    "    y = np.linspace(y-2*sigMax, y+2*sigMax, 100)\n",
    "    \n",
    "    #plot contour pump\n",
    "    contourPlot(axs[1, index], pump_Coeffs[index]['x'][:,0], pump_Coeffs[index]['y'][:,0], x, y, colormap=\"Purples\")\n",
    "    #plot contour probe\n",
    "    contourPlot(axs[1, index], probe_Coeffs[index]['x'][:,0], probe_Coeffs[index]['y'][:,0], x, y, colormap=\"Greens\")\n",
    "    axs[1,index].tick_params(axis='both', which='major', labelsize=7)\n",
    "    axs[1,index].set_xticks([180, 185])\n",
    "    #axs[1,index].set_yticks([165, 170])\n",
    "    textGenerator(axs[2,index], sigbysig[index], dmu[index], correctionFactors[index][0])\n",
    "    #axs[2, index].text(0,0, \"test\")\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#also do the same as below\n",
    "fig2, ax2 = plt.subplots(1,4, dpi=288, figsize=plotHelperLatex.figSizer(1,4))\n",
    "\n",
    "for index, shortPath in enumerate(inputFiles):\n",
    "    #remove legend axis\n",
    "    x = (pump_Coeffs[index]['x'][0,0] + probe_Coeffs[index]['x'][0,0])/2\n",
    "    y = (pump_Coeffs[index]['y'][0,0] + probe_Coeffs[index]['y'][0,0])/2\n",
    "\n",
    "    sigMax = max([pump_Coeffs[index]['x'][1,0], pump_Coeffs[index]['y'][1,0], probe_Coeffs[index]['x'][1,0], probe_Coeffs[index]['y'][1,0]])\n",
    "    x = np.arange(x-2.5*sigMax, x+2.5*sigMax, 1, dtype=int)\n",
    "    y = np.arange(y-2.5*sigMax,y+2.5*sigMax, 1, dtype=int)\n",
    "    \n",
    "    ax2[index].tick_params(axis='both', which='major', labelsize=7)\n",
    "    #plot contour pump\n",
    "    contourPlot(ax2[index], pump_Coeffs[index]['x'][:,0], pump_Coeffs[index]['y'][:,0], x, y, colormap=\"Purples\")\n",
    "    #plot contour probe\n",
    "    contourPlot(ax2[index], probe_Coeffs[index]['x'][:,0], probe_Coeffs[index]['y'][:,0], x, y, colormap=\"Greens\")\n",
    "#plt.tight_layout()\n",
    "\n",
    "\n",
    "#no text but with legend and numeration but with legend\n",
    "fig3, ax3 = plt.subplots(3,4, figsize = plotHelperLatex.figSizer(0.9,4), dpi = 288, gridspec_kw={'height_ratios': [1, 2.5,0.5]})\n",
    "#fig3.subplots_adjust(top = 1, bottom = 0)\n",
    "y_adj = 0.1\n",
    "ax3[0,0].text(0.5,0+y_adj, \"pump pointspread\", verticalalignment =\"center\")\n",
    "ax3[0,0].plot(0,0+y_adj, marker = 's', color = \"purple\")\n",
    "ax3[0,0].text(0.5,0.5+y_adj, \"  probe pointspread\", verticalalignment =\"center\")\n",
    "ax3[0,0].plot(0,0.5+y_adj, marker = 's', color = \"green\")\n",
    "ax3[0,0].set_xlim((-0.5,4.5))\n",
    "ax3[0,0].set_ylim((-0.05,1.0))\n",
    "enumList = [\"a\", \"b\", \"c\", \"d\"]\n",
    "for index, shortPath in enumerate(inputFiles):\n",
    "    #fix the y inversion of the old fits\n",
    "    pump_Coeffs[index]['y'][0,0] = 300-pump_Coeffs[index]['y'][0,0]\n",
    "    probe_Coeffs[index]['y'][0,0] = 300-probe_Coeffs[index]['y'][0,0]\n",
    "\n",
    "    #remove legend axis\n",
    "    ax3[0,index].set_axis_off()\n",
    "    ax3[0,index].set_adjustable('box')\n",
    "    ax3[2,index].set_axis_off()\n",
    "    #find x and y vectors to be used\n",
    "    x = (pump_Coeffs[index]['x'][0,0] + probe_Coeffs[index]['x'][0,0])/2\n",
    "    y = (pump_Coeffs[index]['y'][0,0] + probe_Coeffs[index]['y'][0,0])/2\n",
    "    sigMax = max([pump_Coeffs[index]['x'][1,0], pump_Coeffs[index]['y'][1,0], probe_Coeffs[index]['x'][1,0], probe_Coeffs[index]['y'][1,0]])\n",
    "    x = np.linspace(x-2*sigMax, x+2*sigMax, 100)\n",
    "    #300 for inversion since, old fit files with vertical mirror in artrayanalysis\n",
    "    y = np.linspace(y-2*sigMax, y+2*sigMax, 100)\n",
    "\n",
    "    #text\n",
    "    ax3[2,index].text(0,0, enumList[index]+\")\",verticalalignment=\"center\")\n",
    "    \n",
    "    #plot contour pump\n",
    "    contourPlot(ax3[1, index], pump_Coeffs[index]['x'][:,0], pump_Coeffs[index]['y'][:,0], x, y, colormap=\"Purples\")\n",
    "    #plot contour probe\n",
    "    contourPlot(ax3[1, index], probe_Coeffs[index]['x'][:,0], probe_Coeffs[index]['y'][:,0], x, y, colormap=\"Greens\")\n",
    "    ax3[1,index].tick_params(axis='both', which='major', labelsize=7)\n",
    "    ax3[1,index].set_xticks([180, 185])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(numericCorrectionFactors)\n",
    "for i in range(4):  \n",
    "    print(correctionFactors[i][0][0]*correctionFactors[i][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "from os.path import join\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(1, mainDataDirectoryPath)\n",
    "from peakRadianceAuto import autoPeakRadiance\n",
    "from ArtrayAnalysis import ArtFit\n",
    "#plot the graphs as contour, but not from fits\n",
    "#basing this on rerunFromJSON classmethod of ArtFit\n",
    "def countourComparisonRaw(axes_handle, JSONpath, x, y, bgSubtract = False):\n",
    "    '''currently only for full pump and probe files, implement only single beam later\n",
    "    currently always uses backgroundfile scale'''\n",
    "    print(JSONpath)\n",
    "    f_JSON = open(JSONpath, 'r')\n",
    "    summary_JSON = json.load(f_JSON)\n",
    "    f_JSON.close()\n",
    "    Routine = ArtFit(summary_JSON[\"BackgroundJSON\"], summary_JSON[\"pathProbe\"], summary_JSON[\"pathPump\"], summary_JSON[\"FitAvgWidth / px\"])\n",
    "    probeMap = Routine.probeMap - Routine.bgMap\n",
    "    pumpMap = Routine.pumpMap - Routine.bgMap\n",
    "    \n",
    "    probeMap = np.transpose(probeMap[y[0]:y[-1]+1, x[0]:x[-1]+1])\n",
    "    pumpMap = np.transpose(pumpMap[y[0]:y[-1]+1, x[0]:x[-1]+1])\n",
    "    Xgrid, Ygrid= np.meshgrid(y, x)\n",
    "    #plot probe\n",
    "    axes_handle.contour(Ygrid, Xgrid, probeMap, levels = 5, cmap = \"Greens\")\n",
    "    #plot pump\n",
    "    axes_handle.contour(Ygrid, Xgrid, pumpMap, levels = 5, cmap = \"Purples\")\n",
    "\n",
    "\n",
    "\n",
    "basepath = lambda filename: join(mainDataDirectoryPath , r\"Pump653Probe493_ArtrayCompensationControl\" , filename)\n",
    "inputFiles = [(r\"\\SummaryPump653Probe493Caution.JSON\", r\"\\BeamFitsPump653Probe493Caution.txt\"),\n",
    "              (r\"\\SummaryPump653Probe493_ShiftedProbe.JSON\", r\"\\BeamFitsPump653Probe493_ShiftedProbe.txt\"),\n",
    "              (r\"\\SummaryPump653Probe493_WideProbe.JSON\", r\"\\BeamFitsPump653Probe493_WideProbe.txt\"),\n",
    "              (r\"\\SummaryPump653Probe493_ThinProbe.JSON\", r\"\\BeamFitsPump653Probe493_ThinProbe.txt\")]\n",
    "\n",
    "pump_Coeffs = []\n",
    "probe_Coeffs = []\n",
    "for index, shortPath in enumerate(inputFiles):\n",
    "    dataDict = ArtFit.parseFitFile(basepath(inputFiles[index][1]))\n",
    "    pump_Coeffs.append(dataDict[\"pump\"])\n",
    "    probe_Coeffs.append(dataDict[\"probe\"])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3,4, figsize = plotHelperLatex.figSizer(0.9,3), dpi = 288, gridspec_kw={'height_ratios': [1, 2.5,0.5]})\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "axs[0,0].text(0.5,0, \"pump pointspread\", verticalalignment =\"center\")\n",
    "axs[0,0].plot(0,0, marker = 's', color = \"purple\")\n",
    "axs[0,0].text(0.5,0.05, \"  probe pointspread\", verticalalignment =\"center\")\n",
    "axs[0,0].plot(0,0.05, marker = 's', color = \"green\")\n",
    "axs[0,0].set_xlim((-0.5,4.5))\n",
    "axs[0,0].set_ylim((-0.05,0.1))\n",
    "\n",
    "for index, shortPath in enumerate(inputFiles):\n",
    "    #find x and y vectors to be used\n",
    "    axs[0,index].set_axis_off()\n",
    "    axs[2,index].set_axis_off()\n",
    "    axs[0,index].set_adjustable('box')\n",
    "    x = (pump_Coeffs[index]['x'][0,0] + probe_Coeffs[index]['x'][0,0])/2\n",
    "    y = (pump_Coeffs[index]['y'][0,0] + probe_Coeffs[index]['y'][0,0])/2\n",
    "    sigMax = max([pump_Coeffs[index]['x'][1,0], pump_Coeffs[index]['y'][1,0], probe_Coeffs[index]['x'][1,0], probe_Coeffs[index]['y'][1,0]])\n",
    "    x = np.arange(x-2.5*sigMax, x+2.5*sigMax, 1, dtype=int)\n",
    "    #x = np.arange(0, 300, 1)\n",
    "    y = np.arange(y-2.5*sigMax,y+2.5*sigMax, 1, dtype=int)\n",
    "    #y=np.arange(0,300,1)\n",
    "    #plot both contours\n",
    "    countourComparisonRaw(axs[1,index], basepath(inputFiles[index][0]), x, y)\n",
    "    axs[1,index].tick_params(axis='both', which='major', labelsize=7)\n",
    "    #axs[index].set_xticks([180, 185])\n",
    "    #axs[index].set_yticks([165, 170])\n",
    "    axs[2,index].text(0,0, enumList[index]+\")\",verticalalignment=\"center\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test plotting product of 4 gaussians\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def gaussian(x, popt):\n",
    "    #center, sig\n",
    "    center = popt[0]\n",
    "    sig = popt[1]\n",
    "    return 1/np.sqrt(2*np.pi)/sig*np.exp(-(center-x)**2/(2*sig**2))\n",
    "    \n",
    "#coeffs for gaussian\n",
    "\"coeffs = x_center, sig, Amplitude, offset\"\n",
    "alpha = 15/np.pi\n",
    "coeffsX_1 = [0, 10]\n",
    "coeffsX_2 = [0, 6]\n",
    "coeffsY_1 = [0, 10]\n",
    "coeffsY_2 = [0, 6]\n",
    "x = np.linspace(-30, 30, 100)\n",
    "y = np.linspace(-30, 30, 100)\n",
    "X_grid, Y_grid = np.meshgrid(x, y)\n",
    "\n",
    "Z = gaussian()\n",
    "plt.figure()\n",
    "plt.contour()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fileParsingMethods import numberFileGenerator\n",
    "import sys\n",
    "sys.path.insert(1, mainDataDirectoryPath)\n",
    "from os.path import join\n",
    "import ShowDelayScan\n",
    "\n",
    "#reference responses for 680 are identical at -1.17 mOD\n",
    "\n",
    "fileNames = numberFileGenerator(6692,6696)\n",
    "dirPath = join(mainDataDirectoryPath , r\"SHG-440\\Orthogonal\")\n",
    "ShowDelayScan.fromFiles(fileNames, dirPath, showLegend= True)\n",
    "\n",
    "summaryParallel = join(mainDataDirectoryPath , r\"SHG-440\\saturation_2024-01-16_19-07.mat\")\n",
    "ShowDelayScan.fromFiles(summaryParallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### focal length and effective refractive index for wavelengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import constants as const\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "\n",
    "#Sellmeier equation\n",
    "def n_squared(lamb, coeffs):\n",
    "    '''lamb in µm'''\n",
    "    n2 = 1\n",
    "    lsquare = lamb**2\n",
    "    for index in range(3):\n",
    "        n2 = n2 + coeffs[index, 0]*lsquare/(lsquare-coeffs[index, 1]**2)\n",
    "    return n2\n",
    "\n",
    "#Lensmaker equation\n",
    "def lensmaker(n, R1, R2, d=0):\n",
    "    '''Returns 1/focal length'''\n",
    "    f_inv = (n-1)*(1/R1 - 1/R2 + (n-1)*d/(n*R1*R2))\n",
    "    return f_inv\n",
    "\n",
    "\n",
    "def wavToEnergy(lamb):\n",
    "    #Energy in eV, lamb in nm\n",
    "    return const.h*const.c/lamb*1e9/const.elementary_charge\n",
    "\n",
    "def energyToWav(E):\n",
    "    #Energy in eV, lamb in nm\n",
    "    return const.h*const.c/E*1e9/const.elementary_charge\n",
    "\n",
    "\n",
    "\n",
    "#https://refractiveindex.info/?shelf=glass&book=fused_silica&page=Malitson\n",
    "#Maltison 1965 UV fused silica: 0.21 µm - 3.71 µm\n",
    "coeffsUVFS = np.array([[0.6961663, 0.0684043], [0.4079426, 0.1162414], [0.8974794, 9.896161]])\n",
    "\n",
    "#Energy range is 1.3 eV to 5 eV in .1 eV steps\n",
    "energies = np.linspace(1.3, 5, 37)*const.elementary_charge\n",
    "wavs = const.h*const.c/energies\n",
    "\n",
    "\n",
    "fig1, axs1 = plt.subplots(1,1, layout='constrained', figsize = plotHelperLatex.figSizer(1, 2.2), dpi = 288)\n",
    "#fig2, axs2 = plt.subplots(1,1, layout='constrained', figsize = (6,4), dpi=200)\n",
    "#µm for sellmeier\n",
    "n_wav = np.sqrt(n_squared(wavs*1e6, coeffsUVFS))\n",
    "axs1.plot(wavs*1e9, n_wav)\n",
    "axs1.set_ylabel('n / 1')\n",
    "axs1.set_xlabel(r'$\\lambda$ / nm')\n",
    "axs1.set_xticks(np.arange(250, 951, 100))\n",
    "secax0 = axs1.secondary_xaxis('top', functions=(wavToEnergy, energyToWav))\n",
    "secax0.set_xticks([1.5, 2, 3, 4, 5])\n",
    "secax0.set_xlabel(\"E / eV\")\n",
    "\n",
    "#second part with focal distance\n",
    "R1 = 59.5\n",
    "yaxisNtoF = lambda n_in: -1/lensmaker(n_in, np.inf, R1)\n",
    "#only for the particular case of plano conves\n",
    "yaxisFtoN = lambda f_in: (1+R1/f_in)\n",
    "\n",
    "print(yaxisNtoF(yaxisFtoN(100)))\n",
    "focal_distance = -lensmaker(n_wav, np.inf, R1)\n",
    "focal_distance = 1/focal_distance\n",
    "#axs1.plot(wavs*1e9, yaxisFtoN(focal_distance), 'r')\n",
    "#axs1.set_ylabel('f / mm')\n",
    "#axs1.set_xlabel(r'$\\lambda$ / nm')\n",
    "#axs1.set_xticks(np.arange(250, 951, 100))\n",
    "\n",
    "\n",
    "\n",
    "secax1 = axs1.secondary_yaxis('right', functions=(yaxisNtoF, yaxisFtoN))\n",
    "#secax1.set_xticks([1.5, 2, 3, 4, 5])\n",
    "secax1.set_ylabel('f / mm')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal spot size for wavelengths and displacement from focal spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "#taken from edmund optics: Laser Optics and Resource Guide Section 2: Gaussian Beam Propagation\n",
    "\n",
    "w_c = lambda w0, lamb, z: w0*np.sqrt(1+(lamb*z/(np.pi*w0**2))**2)\n",
    "w0_c = lambda lamb, theta: lamb/np.pi/theta\n",
    "zr_c = lambda lamb, w0: np.pi*w0**2/lamb\n",
    "\n",
    "#collimated beam through lens with focal distance f\n",
    "#taken from angular aperture\n",
    "theta_c = lambda D, f: np.arctan(D/(2*f))\n",
    "\n",
    "\n",
    "\n",
    "wav = 653e-9 # nm\n",
    "#https://refractiveindex.info/?shelf=glass&book=fused_silica&page=Malitson\n",
    "#Maltison 1965 UV fused silica: 0.21 µm - 3.71 µm\n",
    "coeffsUVFS = np.array([[0.6961663, 0.0684043], [0.4079426, 0.1162414], [0.8974794, 9.896161]])\n",
    "n = np.sqrt(n_squared(wav*1e6,coeffsUVFS))\n",
    "f = 1/lensmaker(n, 59.4, np.inf)*1e-3\n",
    "\n",
    "diameters = np.array([1, 2, 3, 4])*1e-3 # mm\n",
    "thet = theta_c(diameters, f)\n",
    "w0 = w0_c(wav, thet)\n",
    "#print(w0*2)\n",
    "\n",
    "zr = zr_c(wav, w0[1])\n",
    "\n",
    "z = np.linspace(-5e-4, zr/2, 200)\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "\n",
    "fig1, axs1 = plt.subplots(1,1, layout='constrained', figsize = plotHelperLatex.figSizer(2,1.4), dpi=144)\n",
    "fig2, axs2 = plt.subplots(1,1, layout='constrained', figsize = plotHelperLatex.figSizer(1.6, 1.4), dpi=144)\n",
    "#fig.suptitle(\"653 nm beam waist\")\n",
    "labelz = z[0]\n",
    "for index in range(len(diameters)):\n",
    "    axs1.plot(z*1e3, w_c(w0[index], wav, z)*2e6, label=\"%.1f mm\" %(diameters[index]*1e3))\n",
    "    axs1.text(labelz*1e3-0.65, w_c(w0[index], wav, labelz)*2e6, s=\"%.1f mm\" %(diameters[index]*1e3), fontdict = {\"fontsize\": \"small\", \"va\": \"center\", \"color\": fig1.gca().lines[-1].get_color()})\n",
    "axs1.set_xlabel(\"z / mm\")\n",
    "axs1.set_ylabel(\"beam waist / µm\")\n",
    "axs1.set_xlim((-1.2, 2))\n",
    "axs1.yaxis.tick_right()\n",
    "axs1.yaxis.set_label_position('right')\n",
    "\n",
    "\n",
    "\n",
    "z = np.linspace(-2e-4, zr/4, 200)\n",
    "\n",
    "labelz = z[-1]\n",
    "for index in range(len(diameters)):\n",
    "    axs2.plot(z*1e3, 1/(w_c(w0[index], wav, z)/w0[index])**2, label=\"%.1f mm\" %(diameters[index]*1e3))\n",
    "    axs2.text(labelz*1e3+0.01, 1/(w_c(w0[index], wav, labelz)/w0[index])**2, s=\"%.1f mm\" %(diameters[index]*1e3), fontdict = {\"fontsize\": \"small\", \"va\": \"center\", \"color\": fig2.gca().lines[-1].get_color()})\n",
    "axs2.set_xlabel(\"z / mm\")\n",
    "axs2.set_ylabel(\"relative maximum intensity / a.u.\")\n",
    "axs2.set_xlim((-0.3,1.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "#taken from edmund optics: Laser Optics and Resource Guide Section 2: Gaussian Beam Propagation\n",
    "\n",
    "w_c = lambda w0, lamb, z: w0*np.sqrt(1+(lamb*z/(np.pi*w0**2))**2)\n",
    "w0_c = lambda lamb, theta: lamb/np.pi/theta\n",
    "zr_c = lambda lamb, w0: np.pi*w0**2/lamb\n",
    "\n",
    "#collimated beam through lens with focal distance f\n",
    "#taken from angular aperture\n",
    "theta_c = lambda D, f: np.arctan(D/(2*f))\n",
    "\n",
    "\n",
    "dia = 2.5e-3 # m\n",
    "wavs = np.array([250,300, 400, 500, 600, 700, 800, 900])*1e-9 #wavs in m\n",
    "n = np.sqrt(n_squared(wav*1e6,coeffsUVFS))\n",
    "f = 1/lensmaker(n, 59.4, np.inf)*1e-3\n",
    "thet = theta_c(dia, f)\n",
    "w0 = w0_c(wavs, thet)\n",
    "\n",
    "zr = np.max(zr_c(wavs, w0))\n",
    "\n",
    "z = np.linspace(-2e-4, zr/4, 200)\n",
    "zlabel = z[-1]\n",
    "plt.figure()\n",
    "fig1, axs1 = plt.subplots(1,1, layout='constrained', figsize = plotHelperLatex.figSizer(2,1.4), dpi=144)\n",
    "#plt.title(\"Beam waist vs wavelength behavior at 2 mm collimated waist\")\n",
    "for index in range(len(wavs)):\n",
    "    axs1.plot(z*1e3, w_c(w0[index], wavs[index], z)*2e6, label=\"%.1f nm\" %(wavs[index]*1e9))\n",
    "    axs1.text(zlabel*1e3+0.01, w_c(w0[index], wavs[index], zlabel)*2e6, s=\"%.0f nm\" %(wavs[index]*1e9), fontdict = {\"fontsize\": \"small\", \"va\": \"center\", \"color\": plt.gca().lines[-1].get_color()})\n",
    "axs1.set_xlabel(\"z / mm\")\n",
    "axs1.set_ylabel(\"beam waist / µm\")\n",
    "axs1.set_xlim((-0.2, 1.05))\n",
    "#plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap normalised by max pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "def gaussian(x, sig = 1, mu = 0):\n",
    "    return 1/(np.sqrt(2*np.pi)*sig)*np.exp(-(x-mu)**2/(2*sig**2))\n",
    "\n",
    "def gaussProd(x, sig1, sig2, mu1, mu2):\n",
    "    mu_n = (mu1*sig2**2 + mu2*sig1**2)/(sig1**2 + sig2**2)\n",
    "    sig_n = sig1*sig2/np.sqrt(sig1**2 + sig2**2)\n",
    "    return 1/(np.sqrt(2*np.pi)*sig_n)*np.exp(-(x-mu_n)**2/(2*sig_n**2))\n",
    "\n",
    "def cFactor(sigPump, sig2, mu1, mu2):\n",
    "    return np.sqrt((sigPump**2 + sig2**2)/sigPump**2)*np.exp((mu1-mu2)**2/(2*(sigPump**2+sig2**2)))\n",
    "\n",
    "\n",
    "\n",
    "def plotVisGaussians(x, sig2, dmu, norm = True, ax=None, colors = [\"blue\", \"royalblue\"]):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "    #probe\n",
    "    #ax.plot(x, gaussian(x, 1, -dmu/2), label = r\"probe: $\\sigma$ = 1; $\\mu = %.1f$\" %(-dmu/2))\n",
    "    #pump r\"pump: $\\sigma$ = %d; $\\mu = %.1f$\" %(sig2, dmu)\n",
    "    ax.plot(x, gaussian(x, sig2, dmu), label = r\"pump\", color = colors[0], linestyle=\"dotted\")\n",
    "    #overlap\n",
    "    if norm == True:\n",
    "        ax.plot(x, gaussProd(x, 1, sig2, -dmu, 0), \"r--\", label = r\"overlap\")\n",
    "    elif norm == False:\n",
    "        relativeGaussiantoProbe = gaussian(x,1,0)*gaussian(x, sig2, dmu)/max(gaussian(x, sig2, +dmu))\n",
    "        #plt.plot(x, relativeGaussiantoProbe, \"r--\", label = r\"overlap\")\n",
    "        ax.fill_between(x, relativeGaussiantoProbe, hatch='/', color=colors[1], facecolor=\"None\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(r\"x / $\\sigma_{probe}$\")\n",
    "    ax.set_ylabel(r\"I / a.u.\")\n",
    "    ax.set_title(r\"$\\frac{\\sigma_{pump}}{\\sigma_{probe}} = %.1d$, $\\Delta\\mu$ = %.1f $\\sigma_{probe}$,  C = %.2f\" %(sig2,dmu, cFactor(sig2, 1, dmu,0)), pad=8)\n",
    "    #plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "x = np.linspace(-3, 3, 200)\n",
    "fig, axs = plt.subplots(3,1, figsize = plotHelperLatex.figSizer(1,1.4), dpi=288, sharex=True)\n",
    "for ver in range(3):\n",
    "    #plot probe first\n",
    "    axs[ver].plot(x, gaussian(x, 1, 0), label = r\"probe\", color=\"teal\")\n",
    "\n",
    "\n",
    "#case 1 µ1 = µ2, sig 1 >> sig 2\n",
    "#\n",
    "sig2 = 1 #times sig 1; sig2 is pump\n",
    "dmu = 1.5\n",
    "plotVisGaussians(x, sig2, dmu, False, ax= axs[1])\n",
    "\n",
    "sig2 = 1 #times sig 1; sig2 is pump\n",
    "dmu = 0\n",
    "plotVisGaussians(x, sig2, dmu, False, colors = [\"orangered\", \"orangered\"], ax=axs[0])\n",
    "\n",
    "sig2 = 5 #times sig 1; sig2 is pump\n",
    "dmu = 0\n",
    "plotVisGaussians(x, sig2, dmu, False, ax= axs[2], colors=[\"purple\", \"purple\"])\n",
    "fig.subplots_adjust(hspace=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "def gaussian(x, sig = 1, mu = 0):\n",
    "    return 1/(np.sqrt(2*np.pi)*sig)*np.exp(-(x-mu)**2/(2*sig**2))\n",
    "\n",
    "def gaussProd(x, sig1, sig2, mu1, mu2):\n",
    "    mu_n = (mu1*sig2**2 + mu2*sig1**2)/(sig1**2 + sig2**2)\n",
    "    sig_n = sig1*sig2/np.sqrt(sig1**2 + sig2**2)\n",
    "    return 1/(np.sqrt(2*np.pi)*sig_n)*np.exp(-(x-mu_n)**2/(2*sig_n**2))\n",
    "\n",
    "def cFactor(sigPump, sig2, mu1, mu2):\n",
    "    return np.sqrt((sigPump**2 + sig2**2)/sigPump**2)*np.exp((mu1-mu2)**2/(2*(sigPump**2+sig2**2)))\n",
    "\n",
    "\n",
    "\n",
    "def plotVisGaussians(x, sig2, dmu):\n",
    "    plt.figure()\n",
    "    #probe\n",
    "    plt.plot(x, gaussian(x, 1, -dmu/2), label = r\"probe: $\\sigma$ = 1; $\\mu = %.1f$\" %(-dmu/2))\n",
    "    #pump\n",
    "    plt.plot(x, gaussian(x, 2, +dmu/2), label = r\"pump: $\\sigma$ = %d; $\\mu = %.1f$\" %(sig2, dmu/2))\n",
    "    #overlap\n",
    "    plt.plot(x, gaussProd(x, 1, sig2, -dmu/2, dmu/2), \"r--\", label = r\"overlap\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(r\"x / $\\sigma$\")\n",
    "    plt.ylabel(r\"I / a.u.\")\n",
    "    plt.title(r\"C = %.2e\" %(cFactor(sig2, 1, dmu,0)))\n",
    "    plt.show()\n",
    "\n",
    "def plotCompareGaussians(x, sigPump, dmu_vec):\n",
    "    fig = plt.figure()\n",
    "    #plot pump gaussian\n",
    "    plt.plot(x, gaussian(x, sigPump, 0))\n",
    "    for ind in range(len(dmu_vec)):\n",
    "        plt.plot(x, gaussian(x, 1, dmu_vec[ind]), label=\"C = %.3f\" %cFactor(sigPump, 1, 0, dmu_vec[ind]))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "\n",
    "#case 1 µ1 = µ2, sig 1 >> sig 2\n",
    "#\n",
    "sig2 = 3 #times sig 1; sig2 is pump\n",
    "dmu = 0 \n",
    "\n",
    "x = np.linspace(-3, 3, 200)\n",
    "dmu = [0, 0.1, 0.3, 0.5]\n",
    "sigPump = 3\n",
    "plotCompareGaussians(x, sigPump, dmu*sigPump)\n",
    "sigPump = 2\n",
    "plotCompareGaussians(x, sigPump, dmu*sigPump)\n",
    "sigPump = 1\n",
    "plotCompareGaussians(x, sigPump, dmu*sigPump)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New binning with interpolation in test_binning_interpol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test binning of numerical correction factor\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "def gaussian(x, sig = 1, mu = 0):\n",
    "    return 1/(np.sqrt(2*np.pi)*sig)*np.exp(-(x-mu)**2/(2*sig**2))\n",
    "\n",
    "def cFactor(sigPump, sig2, mu1, mu2):\n",
    "    return np.sqrt((sigPump**2 + sig2**2)/sigPump**2)*np.exp((mu1-mu2)**2/(2*(sigPump**2+sig2**2)))\n",
    "\n",
    "def binMap(map, xBinning, yBinning=0):\n",
    "    '''N_binning means that N*N pixels are binned together\\n\n",
    "    only takes square z*z maps'''\n",
    "    if yBinning == 0:\n",
    "        yBinning = xBinning\n",
    "    xBins = int(np.shape(map)[0]//xBinning) #along one axis\n",
    "    yBins = int(np.shape(map)[1]//yBinning) #along one axis\n",
    "    outmap = map.reshape(xBins, xBinning, yBins, yBinning).sum(3).sum(1)\n",
    "    return outmap\n",
    "\n",
    "\n",
    "#using identical gaussians for this\n",
    "points_per_sig = 1024 #starting value 1000 points for 1 sig distance\n",
    "sig = 1 #does not matter since I do binning in steps of sig at first\n",
    "mu = 0\n",
    "###----analytical cFactor ----###\n",
    "cFactorAnalytic = cFactor(sig, sig, mu, mu)**2\n",
    "print(\"Analytic correction factor:\\n \" + str(cFactorAnalytic))\n",
    "###----numeric cFactor ----###\n",
    "nsig = 5 # how far I measure to the sides\n",
    "x_like = np.linspace(-nsig*sig,nsig*sig, 2*nsig*points_per_sig)\n",
    "Xvec, Yvec = np.array(np.meshgrid(x_like,x_like))\n",
    "\n",
    "\n",
    "gaussStd = gaussian(Xvec, sig, mu)*gaussian(Yvec, sig, mu)\n",
    "\n",
    "\n",
    "\n",
    "n_steps = 11\n",
    "binning_steps=np.zeros(n_steps, dtype=int)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    binning_steps[i] = 2**i\n",
    "#print(binning_steps)\n",
    "\n",
    "#first without added noise\n",
    "print(\"Correction Factor numeric, no noise:\")\n",
    "noNoiseGaussCorrection = np.zeros(len(binning_steps))\n",
    "for ind in range(len(binning_steps)):\n",
    "    gaussBinned = binMap(gaussStd, binning_steps[ind])\n",
    "    #pump = np.ones(np.shape(gaussBinned))\n",
    "    pump = gaussBinned\n",
    "    probe = gaussBinned\n",
    "    denominator = np.sum(np.multiply(pump, probe)[:])\n",
    "    constantPumpIntensity = np.max(pump)\n",
    "    #print(constantPumpIntensity)\n",
    "    enumerator = np.sum(constantPumpIntensity*probe[:])\n",
    "    #this is missing various constant factors, since this is not using the gaussian shape, but is discretized\n",
    "    noNoiseGaussCorrection[ind] = enumerator/denominator\n",
    "    print(\"binning = 1/\"+str(binning_steps[ind])+\": \" + str(noNoiseGaussCorrection[ind]))\n",
    "\n",
    "plt.figure(dpi = 288, figsize=plotHelperLatex.figSizer(1,2))\n",
    "plt.plot(np.linspace(n_steps-1,0,n_steps), noNoiseGaussCorrection, label = \"no noise\")\n",
    "#with added noise\n",
    "Noiselevels = [0.05,0.1,0.2]\n",
    "NoiseGaussCorrection = np.zeros((len(Noiselevels), len(binning_steps)))\n",
    "for indexSNR, SNR in enumerate(Noiselevels):\n",
    "\n",
    "    gauss1 = gaussStd + (np.random.rand(np.shape(gaussStd)[0], np.shape(gaussStd)[1])-0.5)*np.max(gaussStd)*SNR\n",
    "    gauss2 = gaussStd + (np.random.rand(np.shape(gaussStd)[0], np.shape(gaussStd)[1])-0.5)*np.max(gaussStd)*SNR\n",
    "    \n",
    "    for ind in range(len(binning_steps)):\n",
    "        pump = binMap(gauss1, binning_steps[ind])\n",
    "        probe = binMap(gauss2, binning_steps[ind])\n",
    "        denominator = np.sum(np.multiply(pump, probe)[:])\n",
    "        constantPumpIntensity = np.max(pump)\n",
    "        enumerator = np.sum(constantPumpIntensity*probe[:])\n",
    "        #this is missing various constant factors, since this is not using the gaussian shape, but is discretized\n",
    "        NoiseGaussCorrection[indexSNR, ind] = enumerator/denominator\n",
    "    plt.plot(np.linspace(n_steps-1,0,n_steps), NoiseGaussCorrection[indexSNR], label = \"SNR = %.2f dB\" %(10*np.log10(1/SNR)), linestyle='dashed')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0,n_steps-1], [2,2], label=\"analytical correction\", linestyle=\"dotted\", color = 'red')\n",
    "plt.xlabel(r\"$\\mathrm{log_2}$ of discrete point density per $\\sigma$ / 1\")\n",
    "plt.ylabel(\"correction factor / 1\")\n",
    "plt.ylim([1,2.5])\n",
    "plt.legend(loc=\"lower right\", fontsize = \"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test binning of numerical correction factor\n",
    "#Testing how the offset actually behaves\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotHelperLatex\n",
    "plotHelperLatex.setMatplotSettings()\n",
    "\n",
    "def gaussian(x, sig = 1, mu = 0):\n",
    "    return 1/(np.sqrt(2*np.pi)*sig)*np.exp(-(x-mu)**2/(2*sig**2))\n",
    "\n",
    "def cFactor(sigPump, sig2, mu1, mu2):\n",
    "    return np.sqrt((sigPump**2 + sig2**2)/sigPump**2)*np.exp((mu1-mu2)**2/(2*(sigPump**2+sig2**2)))\n",
    "\n",
    "def binMap(map, xBinning, yBinning=0):\n",
    "    '''N_binning means that N*N pixels are binned together\\n\n",
    "    only takes square z*z maps'''\n",
    "    if yBinning == 0:\n",
    "        yBinning = xBinning\n",
    "    xBins = int(np.shape(map)[0]//xBinning) #along one axis\n",
    "    yBins = int(np.shape(map)[1]//yBinning) #along one axis\n",
    "    outmap = map.reshape(xBins, xBinning, yBins, yBinning).sum(3).sum(1)\n",
    "    return outmap\n",
    "\n",
    "\n",
    "#using identical gaussians for this\n",
    "points_per_sig = 1024 #starting value 1000 points for 1 sig distance\n",
    "sig = 1 #does not matter since I do binning in steps of sig at first\n",
    "dmu = 1\n",
    "###----analytical cFactor ----###\n",
    "#one sigma multiplicator\n",
    "sigPumpYModifier= 2\n",
    "sigPumpXModifier = 0.8\n",
    "cFactorAnalytic = cFactor(sigPumpXModifier*sig, sig, dmu/2, -dmu/2)*cFactor(sigPumpYModifier*sig, sig, dmu/2, -dmu/2)\n",
    "print(\"Analytic correction factor:\\n \" + str(cFactorAnalytic))\n",
    "###----numeric cFactor ----###\n",
    "nsig = 5 # how far I measure to the sides\n",
    "x_like = np.linspace(-nsig*sig,nsig*sig, 2*nsig*points_per_sig)\n",
    "Xvec, Yvec = np.array(np.meshgrid(x_like,x_like))\n",
    "\n",
    "\n",
    "n_steps = 11\n",
    "binning_steps=np.zeros(n_steps, dtype=int)\n",
    "for i in range(n_steps):\n",
    "    binning_steps[i] = 2**i\n",
    "\n",
    "d_offset = [0,0.1,0.2,0.5]\n",
    "GaussCorrection = np.zeros((len(d_offset),len(binning_steps)))\n",
    "for indOffset, offset in enumerate(d_offset):\n",
    "\n",
    "    pumpGauss = gaussian(Xvec, sigPumpXModifier*sig, +dmu/2+offset)*gaussian(Yvec, sigPumpYModifier*sig, +dmu/2)\n",
    "    probeGauss = gaussian(Xvec, sig, -dmu/2+offset)*gaussian(Yvec, sig, -dmu/2)\n",
    "\n",
    "\n",
    "    for ind in range(len(binning_steps)):\n",
    "        pump = binMap(pumpGauss, binning_steps[ind])\n",
    "        probe = binMap(probeGauss, binning_steps[ind])\n",
    "        denominator = np.sum(np.multiply(pump, probe)[:])\n",
    "        constantPumpIntensity = np.max(pump)\n",
    "        enumerator = np.sum(constantPumpIntensity*probe[:])\n",
    "        #this is missing various constant factors, since this is not using the gaussian shape, but is discretized\n",
    "        GaussCorrection[indOffset, ind] = enumerator/denominator\n",
    "\n",
    "\n",
    "plt.figure(dpi = 288, figsize=plotHelperLatex.figSizer(1,2))\n",
    "for i in range(len(d_offset)):\n",
    "    plt.plot(np.linspace(n_steps-1,0,n_steps), GaussCorrection[i], label = r\"$\\Delta x_\\mathrm{grid}$ = %.1f $\\sigma$\" %(d_offset[i]))\n",
    "plt.plot([0,n_steps-1], [cFactorAnalytic,cFactorAnalytic], label=\"analytical correction\", linestyle=\"dotted\", color = 'red')\n",
    "plt.xlabel(r\"$\\mathrm{log_2}$ of discrete point density per $\\sigma$ / 1\")\n",
    "plt.ylabel(\"correction factor / 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
